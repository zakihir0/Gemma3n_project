"""
Training Modules
Vision Fine-tuningé–¢é€£ã®æ©Ÿèƒ½
"""

import random
import torch
import torch.nn.functional as F
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datasets import Dataset
from PIL import Image
from unsloth import FastVisionModel
from unsloth.trainer import UnslothVisionDataCollator
from trl import SFTTrainer, SFTConfig

class MushroomVisionDataset:
    """ã‚­ãƒã‚³ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, image_paths: List[str], labels: List[str], class_names: List[str]):
        """
        Args:
            image_paths: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            labels: ã‚¯ãƒ©ã‚¹åã®ãƒªã‚¹ãƒˆ
            class_names: å…¨ã‚¯ãƒ©ã‚¹åã®ãƒªã‚¹ãƒˆ
        """
        self.image_paths = image_paths
        self.labels = labels
        self.class_names = class_names
        self.label_to_id = {name: i for i, name in enumerate(class_names)}
    
    def prepare_vision_dataset(self, train_ratio: float = 0.8, max_samples_per_class: int = 10) -> tuple:
        """
        Vision fine-tuningç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
        
        Args:
            train_ratio: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ¯”ç‡
            max_samples_per_class: ã‚¯ãƒ©ã‚¹ã‚ãŸã‚Šã®æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°
            
        Returns:
            (train_dataset, val_dataset)
        """
        print(f"ğŸ“Š Vision fine-tuningç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™é–‹å§‹...")
        
        # ã‚¯ãƒ©ã‚¹åˆ¥ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†
        class_data = {}
        for path, label in zip(self.image_paths, self.labels):
            if label not in class_data:
                class_data[label] = []
            class_data[label].append(path)
        
        # å„ã‚¯ãƒ©ã‚¹ã‹ã‚‰å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        selected_data = []
        for class_name, paths in class_data.items():
            if len(paths) > 0:
                # ã‚¯ãƒ©ã‚¹ã‚ãŸã‚Šã®æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°ã«åˆ¶é™
                sampled_paths = random.sample(paths, min(len(paths), max_samples_per_class))
                for path in sampled_paths:
                    # Vision Fine-tuningç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼
                    messages = [{
                        "role": "user",
                        "content": [
                            {"type": "image", "image": path},
                            {"type": "text", "text": f"What type of mushroom is this? Identify this mushroom."}
                        ]
                    }, {
                        "role": "assistant", 
                        "content": f"This is a {class_name} mushroom."
                    }]
                    
                    selected_data.append({
                        'image_path': path,
                        'class_name': class_name,
                        'class_id': self.label_to_id.get(class_name, -1),
                        'messages': messages,
                        'text': f"This is a {class_name} mushroom."  # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç”¨
                    })
        
        print(f"   é¸æŠã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿æ•°: {len(selected_data)}")
        print(f"   å¯¾è±¡ã‚¯ãƒ©ã‚¹æ•°: {len(class_data)}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        random.shuffle(selected_data)
        
        # è¨“ç·´/æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
        split_idx = int(len(selected_data) * train_ratio)
        train_data = selected_data[:split_idx]
        val_data = selected_data[split_idx:]
        
        print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_data)}")
        print(f"   æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_data)}")
        
        # Hugging Face Datasetå½¢å¼ã«å¤‰æ›
        train_dataset = Dataset.from_list(train_data)
        val_dataset = Dataset.from_list(val_data)
        
        return train_dataset, val_dataset

def formatting_func_for_vision(examples):
    """Vision fine-tuningç”¨ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆé–¢æ•°"""
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¿”ã™
    return examples

class MushroomVisionFineTuner:
    """ã‚­ãƒã‚³ç”»åƒåˆ†é¡ç”¨Vision Fine-tunerã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, base_model, base_tokenizer):
        """
        Args:
            base_model: ãƒ™ãƒ¼ã‚¹ã®Gemma3nãƒ¢ãƒ‡ãƒ«
            base_tokenizer: ãƒ™ãƒ¼ã‚¹ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        """
        self.base_model = base_model
        self.base_tokenizer = base_tokenizer
        self.vision_model = None
        self.is_fine_tuned = False
    
    def setup_vision_model(self, finetune_vision_layers: bool = True, 
                          finetune_language_layers: bool = False):
        """
        Vision fine-tuningç”¨ãƒ¢ãƒ‡ãƒ«è¨­å®š
        
        Args:
            finetune_vision_layers: Visionå±¤ã‚’fine-tuningã™ã‚‹ã‹
            finetune_language_layers: Languageå±¤ã‚’fine-tuningã™ã‚‹ã‹
        """
        print(f"ğŸ”§ Vision fine-tuningç”¨ãƒ¢ãƒ‡ãƒ«è¨­å®šä¸­...")
        
        try:
            # FastVisionModelã§PEFTè¨­å®š
            self.vision_model = FastVisionModel.get_peft_model(
                self.base_model,
                finetune_vision_layers=finetune_vision_layers,
                finetune_language_layers=finetune_language_layers,
                finetune_attention_modules=True,
                finetune_mlp_modules=True,
                r=16,  # LoRA rank
                lora_alpha=32,
                lora_dropout=0.1,
                target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
                use_gradient_checkpointing="unsloth",
                random_state=42,
            )
            # Trainingç”¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’æœ‰åŠ¹åŒ–ï¼ˆé‡è¦ï¼‰
            FastVisionModel.for_training(self.vision_model)
            
            print(f"âœ… Vision modelè¨­å®šå®Œäº†")
            print(f"   Visionå±¤ fine-tuning: {finetune_vision_layers}")
            print(f"   Languageå±¤ fine-tuning: {finetune_language_layers}")
            print(f"   Training modeæœ‰åŠ¹åŒ–: âœ…")
            
        except Exception as e:
            print(f"âŒ Vision modelè¨­å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise e
    
    def train_vision_model(self, train_dataset, val_dataset, 
                          output_dir: str = "./mushroom_vision_model",
                          num_epochs: int = 3,
                          batch_size: int = 2,
                          learning_rate: float = 2e-5):
        """
        Vision modelã‚’è¨“ç·´
        
        Args:
            train_dataset: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            val_dataset: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            output_dir: ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            num_epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            learning_rate: å­¦ç¿’ç‡
        """
        if self.vision_model is None:
            raise ValueError("Vision modelãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚setup_vision_model()ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        
        print(f"ğŸš€ Vision modelè¨“ç·´é–‹å§‹...")
        print(f"   ã‚¨ãƒãƒƒã‚¯æ•°: {num_epochs}")
        print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
        print(f"   å­¦ç¿’ç‡: {learning_rate}")
        
        try:
            # è¨“ç·´å¼•æ•°è¨­å®šï¼ˆSFTConfigä½¿ç”¨ï¼‰
            training_args = SFTConfig(
                output_dir=output_dir,
                num_train_epochs=num_epochs,
                per_device_train_batch_size=batch_size,
                per_device_eval_batch_size=batch_size,
                gradient_accumulation_steps=4,  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
                learning_rate=learning_rate,
                warmup_steps=50,
                logging_steps=10,
                eval_strategy="steps",
                eval_steps=50,
                save_strategy="steps",
                save_steps=100,
                load_best_model_at_end=True,
                metric_for_best_model="eval_loss",
                greater_is_better=False,
                # Visionç‰¹æœ‰ã®è¨­å®š
                remove_unused_columns=False,
                dataset_text_field="",
                dataset_kwargs={"skip_prepare_dataset": True},
                dataloader_pin_memory=False,
                fp16=True,
                optim="adamw_8bit",  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
                weight_decay=0.01,
                lr_scheduler_type="linear",
                report_to="none",  # WandBãªã©ã®ãƒ­ã‚°ã‚’ç„¡åŠ¹
            )
            
            # SFTTrainerè¨­å®š
            trainer = SFTTrainer(
                model=self.vision_model,
                tokenizer=self.base_tokenizer,
                train_dataset=train_dataset,
                eval_dataset=val_dataset,
                data_collator=UnslothVisionDataCollator(model=self.vision_model, processor=self.base_tokenizer),
                args=training_args,
                max_seq_length=512,
                # Visionç”¨ã®ç‰¹åˆ¥è¨­å®š
                dataset_text_field="messages",  # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŒ‡å®š
                packing=False,  # Visionç”¨ã«ãƒ‘ãƒƒã‚­ãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–
            )
            
            # è¨“ç·´å®Ÿè¡Œ
            print(f"ğŸ“ˆ è¨“ç·´é–‹å§‹...")
            trainer.train()
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            print(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­: {output_dir}")
            trainer.save_model(output_dir)
            self.base_tokenizer.save_pretrained(output_dir)
            
            self.is_fine_tuned = True
            print(f"âœ… Vision modelè¨“ç·´å®Œäº†ï¼")
            
            # è¨“ç·´çµæœã‚µãƒãƒªãƒ¼
            train_result = trainer.state.log_history
            if train_result:
                final_loss = train_result[-1].get('eval_loss', 'N/A')
                print(f"   æœ€çµ‚è©•ä¾¡æå¤±: {final_loss}")
            
        except Exception as e:
            print(f"âŒ è¨“ç·´ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise e
    
    def load_fine_tuned_model(self, model_path: str):
        """
        Fine-tunedãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        
        Args:
            model_path: ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        """
        try:
            print(f"ğŸ“‚ Fine-tunedãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­: {model_path}")
            
            # FastVisionModelã§ãƒ­ãƒ¼ãƒ‰
            self.vision_model, self.base_tokenizer = FastVisionModel.from_pretrained(
                model_name=model_path,
                dtype=None,
                max_seq_length=1024,
                load_in_4bit=True,
            )
            
            self.is_fine_tuned = True
            print(f"âœ… Fine-tunedãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise e
    
    def get_model_for_inference(self):
        """æ¨è«–ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—"""
        if self.is_fine_tuned and self.vision_model is not None:
            return self.vision_model
        else:
            return self.base_model

def run_mushroom_vision_finetuning(image_paths: List[str], labels: List[str], 
                                  class_names: List[str], base_model, base_tokenizer,
                                  output_dir: str = "./mushroom_vision_finetuned",
                                  max_samples_per_class: int = 5, num_epochs: int = 2):
    """
    ã‚­ãƒã‚³ç”»åƒã§ã®Vision Fine-tuningã‚’ç‹¬ç«‹å®Ÿè¡Œ
    
    Args:
        image_paths: ç”»åƒãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        labels: ãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆ
        class_names: ã‚¯ãƒ©ã‚¹åã®ãƒªã‚¹ãƒˆ
        base_model: ãƒ™ãƒ¼ã‚¹Gemma3nãƒ¢ãƒ‡ãƒ«
        base_tokenizer: ãƒ™ãƒ¼ã‚¹ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        output_dir: ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        max_samples_per_class: ã‚¯ãƒ©ã‚¹ã‚ãŸã‚Šã®æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°
        num_epochs: ã‚¨ãƒãƒƒã‚¯æ•°
    
    Returns:
        Fine-tunedãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
    """
    print(f"ğŸš€ ã‚­ãƒã‚³ç”»åƒVision Fine-tuningé–‹å§‹...")
    print(f"   å¯¾è±¡ç”»åƒæ•°: {len(image_paths)}")
    print(f"   ã‚¯ãƒ©ã‚¹æ•°: {len(class_names)}")
    print(f"   ã‚¯ãƒ©ã‚¹ã‚ãŸã‚Šæœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°: {max_samples_per_class}")
    print(f"   ã‚¨ãƒãƒƒã‚¯æ•°: {num_epochs}")
    print(f"   ä¿å­˜å…ˆ: {output_dir}")
    
    try:
        # Step 1: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
        print(f"\nğŸ“Š Step 1: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™")
        dataset_manager = MushroomVisionDataset(image_paths, labels, class_names)
        train_dataset, val_dataset = dataset_manager.prepare_vision_dataset(
            max_samples_per_class=max_samples_per_class
        )
        
        # Step 2: Fine-tunerã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        print(f"\nğŸ”§ Step 2: Fine-tuneråˆæœŸåŒ–")
        finetuner = MushroomVisionFineTuner(base_model, base_tokenizer)
        finetuner.setup_vision_model(
            finetune_vision_layers=True,
            finetune_language_layers=False
        )
        
        # Step 3: è¨“ç·´å®Ÿè¡Œ
        print(f"\nğŸ“ˆ Step 3: è¨“ç·´å®Ÿè¡Œ")
        finetuner.train_vision_model(
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            output_dir=output_dir,
            num_epochs=num_epochs,
            batch_size=1,  # VRAMåˆ¶ç´„ã®ãŸã‚1ã«è¨­å®š
            learning_rate=2e-5
        )
        
        print(f"\nğŸ‰ Vision Fine-tuningå®Œäº†ï¼")
        print(f"   ä¿å­˜å…ˆ: {output_dir}")
        
        return finetuner.get_model_for_inference(), base_tokenizer
        
    except Exception as e:
        print(f"\nâŒ Fine-tuningå¤±æ•—: {str(e)}")
        raise e

def create_workflow_with_finetuned_model(finetuned_model, tokenizer, embedding_model):
    """
    Fine-tunedãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä½œæˆ
    
    Args:
        finetuned_model: Fine-tunedãƒ¢ãƒ‡ãƒ«
        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        embedding_model: Embedingãƒ¢ãƒ‡ãƒ«
        
    Returns:
        Fine-tunedãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
    """
    print(f"ğŸ”„ Fine-tunedãƒ¢ãƒ‡ãƒ«ã§ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä½œæˆä¸­...")
    
    # Fine-tunedãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä½œæˆ
    from .rag_workflow import MobileMushroomWorkflow
    
    workflow = MobileMushroomWorkflow(
        model=finetuned_model,
        tokenizer=tokenizer,
        embedding_model=embedding_model
    )
    
    print(f"âœ… Fine-tunedãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä½œæˆå®Œäº†")
    return workflow