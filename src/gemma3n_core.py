"""
Gemma3n Core Module
ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã€ã‚¤ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã€ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®æ ¸ã¨ãªã‚‹æ©Ÿèƒ½
"""

import os
import pandas as pd
import random
import numpy as np
import json
import time
import torch
from typing import Optional, List, Dict, Any, Tuple
from unsloth import FastModel
from transformers import TextStreamer
from torchvision import datasets, transforms, models

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
model = None
tokenizer = None

def initialize_model():
    """Gemma3nãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’åˆæœŸåŒ–"""
    global model, tokenizer
    
    if model is None or tokenizer is None:
        print("ğŸš€ Gemma3nãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
        model, tokenizer = FastModel.from_pretrained(
            model_name="unsloth/gemma-3n-E2B-it",
            dtype=None,
            max_seq_length=1024,
            load_in_4bit=True,
            full_finetuning=False,
        )
        print("âœ… ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†")
    
    return model, tokenizer

def do_gemma_3n_inference(messages, max_new_tokens=128):
    """Gemma3nã§ã‚¤ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã‚’å®Ÿè¡Œ"""
    global model, tokenizer
    
    # ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã¯åˆæœŸåŒ–
    if model is None or tokenizer is None:
        model, tokenizer = initialize_model()
    
    inputs = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt",
    ).to("cuda")
    
    # å¸¸ã«å‡ºåŠ›ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ã¦è¿”ã™
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=1.0, top_p=0.95, top_k=64,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
    )
    # æ–°ã—ãç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã‚’æŠ½å‡º
    response_tokens = outputs[0][inputs['input_ids'].shape[1]:]
    response_text = tokenizer.decode(response_tokens, skip_special_tokens=True)
    return response_text

def load_preprocessed_data():
    """å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’é«˜é€Ÿèª­ã¿è¾¼ã¿"""
    try:
        if (os.path.exists('/kaggle/working/mushroom_dataset.parquet') and 
            os.path.exists('/kaggle/working/mushroom_metadata.json') and
            os.path.exists('/kaggle/working/mushroom_path_label.parquet')):
            
            print("ğŸ“‚ å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
            data = pd.read_parquet('/kaggle/working/mushroom_dataset.parquet')
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            with open('/kaggle/working/mushroom_metadata.json', 'r') as f:
                metadata = json.load(f)
            
            # path_labelèª­ã¿è¾¼ã¿
            path_label_df = pd.read_parquet('/kaggle/working/mushroom_path_label.parquet')
            path_label = list(zip(path_label_df['path'], path_label_df['label']))
            
            print(f"âœ… å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†:")
            print(f"   ğŸ“Š Dataset: {len(data)} records")
            print(f"   ğŸ·ï¸ Classes: {metadata['total_classes']} classes")
            print(f"   ğŸ”— Path-Label: {len(path_label)} records")
            
            return data, metadata, path_label
        else:
            return None, None, None
            
    except Exception as e:
        print(f"âš ï¸ å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {str(e)}")
        return None, None, None

def process_dataset(dir0, max_files_per_class=1000):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å‰å‡¦ç†"""
    print("ğŸ”„ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‰å‡¦ç†ã‚’å®Ÿè¡Œä¸­...")
    
    classes = []
    paths = []
    class_file_counts = {}
    
    for dirname, _, filenames in os.walk(dir0):
        class_name = dirname.split('/')[-1]
        if class_name not in class_file_counts:
            class_file_counts[class_name] = 0
        
        for filename in filenames:
            if class_file_counts[class_name] < max_files_per_class:
                classes += [class_name]
                paths += [(os.path.join(dirname, filename))]
                class_file_counts[class_name] += 1

    dataset0 = datasets.ImageFolder(root=dir0)
    class_names = dataset0.classes
    print(class_names)
    print(len(class_names))

    N = list(range(len(classes)))
    normal_mapping = dict(zip(class_names, N))
    reverse_mapping = dict(zip(N, class_names))

    data = pd.DataFrame(columns=['path', 'class', 'label'])
    data['path'] = paths
    data['class'] = classes
    data['label'] = data['class'].map(normal_mapping)
    print(len(data))

    def create_path_label_list(df):
        path_label_list = []
        for _, row in df.iterrows():
            path = row['path']
            label = row['label']
            path_label_list.append((path, label))
        return path_label_list

    path_label = create_path_label_list(data)
    path_label = random.sample(path_label, 20000)
    print(len(path_label))
    print(path_label[0:3])

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜ï¼ˆåŠ¹ç‡åŒ–ã®ãŸã‚ï¼‰
    print("\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜ä¸­...")
    try:
        # Parquetå½¢å¼ã§ä¿å­˜
        data.to_parquet('/kaggle/working/mushroom_dataset.parquet', index=False)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        metadata = {
            'class_names': class_names,
            'normal_mapping': normal_mapping,
            'reverse_mapping': reverse_mapping,
            'total_images': len(data),
            'total_classes': len(class_names),
            'path_label_sample_size': len(path_label)
        }
        
        with open('/kaggle/working/mushroom_metadata.json', 'w') as f:
            json.dump(metadata, f, indent=2)
        
        # path_labelã‚‚ä¿å­˜
        path_label_df = pd.DataFrame(path_label, columns=['path', 'label'])
        path_label_df.to_parquet('/kaggle/working/mushroom_path_label.parquet', index=False)
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜å®Œäº†:")
        print(f"   ğŸ“Š Dataset: /kaggle/working/mushroom_dataset.parquet ({len(data)} records)")
        print(f"   ğŸ·ï¸ Metadata: /kaggle/working/mushroom_metadata.json")
        print(f"   ğŸ”— Path-Label: /kaggle/working/mushroom_path_label.parquet ({len(path_label)} records)")
        
    except Exception as e:
        print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜å¤±æ•—: {str(e)}")
    
    return data, class_names, normal_mapping, reverse_mapping, path_label

def get_model_tokenizer():
    """ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å–å¾—ï¼ˆå¤–éƒ¨ã‚¢ã‚¯ã‚»ã‚¹ç”¨ï¼‰"""
    global model, tokenizer
    if model is None or tokenizer is None:
        return initialize_model()
    return model, tokenizer