#%%
# Vision Fine-tuning for plnat Classification - CLAUDE.md Based

!pip uninstall -y pygobject gradient gradient-utils
!pip install --no-cache-dir --upgrade packaging==24.2
!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo
!pip install --upgrade bitsandbytes
!pip install triton==3.2.0
!pip install pip3-autoremove
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
!pip install unsloth
!pip install faiss-cpu
!pip install sentence-transformers
!pip install wikipedia
!pip install --no-deps git+https://github.com/huggingface/transformers.git
!pip install --no-deps --upgrade timm
!pip uninstall -y deepspeed
!pip install deepspeed==0.14.4
!pip install wandb==0.17.9

#%%
import os
import random
import pandas as pd
import json
from typing import List
from collections import defaultdict
from torchvision import datasets
from unsloth import FastModel, is_bf16_supported, FastVisionModel
from unsloth.trainer import UnslothVisionDataCollator
from trl import SFTTrainer, SFTConfig
import wandb
from PIL import Image
from IPython.display import display, Image as IPImage

os.environ["CUDA_VISIBLE_DEVICES"] = "0, 1"
os.environ["WANDB_API_KEY"] = "abd0ce2837eca439d66f6c603136603c1729cd3e"

#%%
# Initialize WandB with environment variable login
wandb.login(key=os.getenv("WANDB_API_KEY"))

wandb.init(
    project="gemma3n-plant-classification",
    name="vision-finetuning-experiment",
    config={
        "model_name": "unsloth/gemma-3n-E2B-it",
        "max_seq_length": 1024,
        "load_in_4bit": True,
        "max_files_per_class_train": 10000,
        "max_files_per_class_val": 50,
        "max_val_samples_for_sft": 200,
        "lora_rank": 32,
        "lora_alpha": 64,
        "learning_rate": 1e-4,
        "max_steps": 100,
        "eval_steps": 10,
        "logging_steps": 1,
        "per_device_train_batch_size": 8,
        "per_device_eval_batch_size": 16,
        "gradient_accumulation_steps": 8,
    }
)

# Load the model (exactly as in CLAUDE.md)
model, tokenizer = FastModel.from_pretrained(
    model_name = "unsloth/gemma-3n-E2B-it",
    dtype = None,
    max_seq_length = 1024,
    load_in_4bit = True,
    full_finetuning = False,
)

#%%
# Data loading using existing train/valid split
base_dir = '/notebooks/kaggle/input/new_plant_diseases/2/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)'
train_dir = os.path.join(base_dir, 'train')
valid_dir = os.path.join(base_dir, 'valid')

# Dataset size configuration
max_files_per_class_train = 10000  # Training data limit
max_files_per_class_val = 50       # Validation data limit (small for faster evaluation)

print(f"üìä Dataset configuration:")
print(f"   Training: max {max_files_per_class_train} files per class")
print(f"   Validation: max {max_files_per_class_val} files per class")

def load_dataset_from_dir(root_dir, max_files_per_class=None):
    """Load dataset from directory with optional file limit per class"""
    classes = []
    paths = []
    class_file_counts = {}
    
    for dirname, _, filenames in os.walk(root_dir):
        class_name = dirname.split('/')[-1]
        if class_name == os.path.basename(root_dir):  # Skip root directory
            continue
            
        if class_name not in class_file_counts:
            class_file_counts[class_name] = 0
        
        for filename in filenames:
            if max_files_per_class is None or class_file_counts[class_name] < max_files_per_class:
                if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):
                    full_path = os.path.join(dirname, filename)
                    if os.path.exists(full_path):
                        classes.append(class_name)
                        paths.append(full_path)
                        class_file_counts[class_name] += 1
    
    return classes, paths, class_file_counts

# Load train and validation datasets separately with different limits
train_classes, train_paths, train_counts = load_dataset_from_dir(train_dir, max_files_per_class_train)
val_classes, val_paths, val_counts = load_dataset_from_dir(valid_dir, max_files_per_class_val)

# Get unified class names from ImageFolder for consistency
train_dataset_folder = datasets.ImageFolder(root=train_dir)
class_names = train_dataset_folder.classes
print(f"Class names: {class_names}")
print(f"Number of classes: {len(class_names)}")

# Create mapping
N = list(range(len(class_names)))
normal_mapping = dict(zip(class_names, N))
reverse_mapping = dict(zip(N, class_names))

# Create train dataset
train_dataset = []
for path, class_name in zip(train_paths, train_classes):
    train_dataset.append({"path": path, "class": class_name})

# Create validation dataset
val_dataset = []
for path, class_name in zip(val_paths, val_classes):
    val_dataset.append({"path": path, "class": class_name})

print(f"Training samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")
print(f"Train class distribution: {train_counts}")
print(f"Val class distribution: {val_counts}")

#%%
# Add special tokens for plant classes
print(f"\nüè∑Ô∏è Adding special tokens for {len(class_names)} plant classes...")
print(f"üîç Investigating tokenizer type: {type(tokenizer)}")

# Check tokenizer attributes
print(f"üìã Available tokenizer attributes: {[attr for attr in dir(tokenizer) if not attr.startswith('_')]}")

# STEP 1: Investigate current vocabulary structure and token ID ranges
print(f"\nüî¨ DETAILED VOCABULARY ANALYSIS")
print("=" * 60)

# Check base vocabulary size
if hasattr(tokenizer, 'tokenizer'):
    actual_tokenizer = tokenizer.tokenizer
    base_vocab_size = len(actual_tokenizer)
    print(f"üìä Base vocabulary size (underlying tokenizer): {base_vocab_size}")
else:
    base_vocab_size = len(tokenizer)
    actual_tokenizer = tokenizer
    print(f"üìä Base vocabulary size (direct tokenizer): {base_vocab_size}")

# Investigate model configuration for multimodal embeddings
print(f"\nüß† Model Configuration Analysis:")
if hasattr(model, 'config'):
    config = model.config
    print(f"   Model type: {getattr(config, 'model_type', 'Unknown')}")
        
    # Check for multimodal specific configurations
    text_config = getattr(config, 'text_config', None)
    if text_config:
        print(f"   Text vocab size: {getattr(text_config, 'vocab_size', 'Not found')}")
        print(f"   Text vocab offset: {getattr(text_config, 'vocab_offset', 'Not found')}")

    audio_config = getattr(config, 'audio_config', None)
    if audio_config:
        print(f"   Audio vocab size: {getattr(audio_config, 'vocab_size', 'Not found')}")
        print(f"   Audio vocab offset: {getattr(audio_config, 'vocab_offset', 'Not found')}")
    
    vision_config = getattr(config, 'vision_config', None)
    if vision_config:
        print(f"   Vision vocab size: {getattr(vision_config, 'vocab_size', 'Not found')}")
        print(f"   Vision vocab offset: {getattr(vision_config, 'vocab_offset', 'Not found')}")

# Check for special token ranges in current tokenizer
print(f"\nüéØ Special Token Investigation:")
if hasattr(actual_tokenizer, 'all_special_tokens'):
    existing_special_tokens = actual_tokenizer.all_special_tokens
    print(f"   Existing special tokens: {len(existing_special_tokens)}")
    print(f"   Examples: {existing_special_tokens[:10]}")
    
    # Get IDs of existing special tokens
    if existing_special_tokens:
        special_token_ids = [actual_tokenizer.convert_tokens_to_ids(token) for token in existing_special_tokens[:5]]
        print(f"   Special token ID examples: {special_token_ids}")
        print(f"   Special token ID range: {min(special_token_ids) if special_token_ids else 'N/A'} - {max(special_token_ids) if special_token_ids else 'N/A'}")

# Sample some token IDs to understand the vocabulary layout
print(f"\nüîç Token ID Sampling:")
sample_tokens = ['<', '>', 'plant', 'class', 'the', 'a']
for token in sample_tokens:
    try:
        token_id = actual_tokenizer.convert_tokens_to_ids(token)
        print(f"   '{token}' ‚Üí ID: {token_id}")
    except:
        print(f"   '{token}' ‚Üí Not found")

# Test a range of high token IDs to find safe ranges
print(f"\nüé≤ High Token ID Range Test:")
test_ranges = [base_vocab_size - 10, base_vocab_size, base_vocab_size + 100, base_vocab_size + 1000]
for test_id in test_ranges:
    try:
        if test_id < len(actual_tokenizer):
            token = actual_tokenizer.convert_ids_to_tokens(test_id)
            print(f"   ID {test_id}: '{token}'")
        else:
            print(f"   ID {test_id}: Out of range (max: {len(actual_tokenizer)-1})")
    except:
        print(f"   ID {test_id}: Error accessing")

special_tokens = [f"<PLANT_CLASS_{i}>" for i in range(len(class_names))]
print(f"\nüìã Planned special tokens: {len(special_tokens)} tokens")
print(f"   Will be added to range: {base_vocab_size} - {base_vocab_size + len(special_tokens) - 1}")

# SAFETY CHECK: Ensure we're not conflicting with known ranges
print(f"\n‚ö†Ô∏è  SAFETY ANALYSIS:")
if base_vocab_size + len(special_tokens) > 300000:  # Reasonable upper bound
    print(f"   ‚ùå WARNING: New vocab size would be {base_vocab_size + len(special_tokens)}, which seems too large")
    print(f"   üîÑ Consider reducing number of classes or using different approach")
else:
    print(f"   ‚úÖ New vocab size {base_vocab_size + len(special_tokens)} appears reasonable")

# STEP 2: Safe special token addition with conflict avoidance
print(f"\nüîß SAFE SPECIAL TOKEN ADDITION")
print("=" * 60)

try:
    # Check if we should proceed based on safety analysis
    if base_vocab_size + len(special_tokens) > 300000:
        raise ValueError(f"Vocabulary would become too large: {base_vocab_size + len(special_tokens)}")
    
    # Store original vocab size for verification
    original_vocab_size = len(actual_tokenizer)
    print(f"üìä Original vocabulary size: {original_vocab_size}")
    
    # Step 1: Add special tokens to the underlying tokenizer first
    if hasattr(tokenizer, 'tokenizer'):
        print("üîß Found tokenizer.tokenizer - using underlying tokenizer")
        
        # Use add_special_tokens for proper special token handling
        num_added = actual_tokenizer.add_special_tokens({"additional_special_tokens": special_tokens})
        print(f"‚úÖ Added {num_added} special tokens to underlying tokenizer")
        
        # Verify the token IDs are in safe range
        new_vocab_size = len(actual_tokenizer)
        print(f"üìä New vocabulary size: {new_vocab_size}")
        
        # Check token ID assignments
        added_token_ids = []
        for token in special_tokens:
            token_id = actual_tokenizer.convert_tokens_to_ids(token)
            added_token_ids.append(token_id)
        
        print(f"üéØ Added token ID range: {min(added_token_ids)} - {max(added_token_ids)}")
        
        # CRITICAL: Check if any new token IDs conflict with known ranges
        conflict_detected = False
        
        # Check for low-range conflicts (0-127 = audio range from error)
        low_range_conflicts = [tid for tid in added_token_ids if tid < 128]
        if low_range_conflicts:
            print(f"‚ùå CONFLICT DETECTED: Token IDs in audio range (0-127): {low_range_conflicts}")
            conflict_detected = True
        
        # Check for unexpected ID patterns
        if max(added_token_ids) < original_vocab_size:
            print(f"‚ùå CONFLICT DETECTED: New tokens assigned to existing vocab range")
            conflict_detected = True
        
        if conflict_detected:
            print(f"üîÑ Conflict detected, removing added tokens and falling back...")
            # Note: Cannot easily remove tokens, so we'll fall back to text classification
            raise ValueError("Token ID conflicts detected")
        
        # Step 2: Resize model embeddings with extra safety checks
        print(f"üîÑ Resizing model embeddings from {original_vocab_size} to {new_vocab_size}...")
        try:
            model.resize_token_embeddings(new_vocab_size)
            print("‚úÖ Successfully resized model embeddings")
            
            # Verify resize worked
            if hasattr(model, 'get_input_embeddings'):
                embedding_size = model.get_input_embeddings().num_embeddings
                print(f"üìä Model embedding size after resize: {embedding_size}")
                if embedding_size != new_vocab_size:
                    print(f"‚ö†Ô∏è Warning: Embedding size mismatch ({embedding_size} vs {new_vocab_size})")
            
        except Exception as resize_e:
            print(f"‚ùå Resize error: {resize_e}")
            raise resize_e
        
        vocab_size = new_vocab_size
        print("‚úÖ Successfully added tokens via underlying tokenizer")
    
    # Fallback options if tokenizer.tokenizer is not available
    elif hasattr(tokenizer, 'add_special_tokens'):
        print("üîß Using direct add_special_tokens")
        num_added = tokenizer.add_special_tokens({"additional_special_tokens": special_tokens})
        print(f"‚úÖ Added {num_added} special tokens")
        model.resize_token_embeddings(len(tokenizer))
        vocab_size = len(tokenizer)
        print("‚úÖ Successfully added tokens via add_special_tokens")
    
    elif hasattr(tokenizer, 'add_tokens'):
        print("üîß Using tokenizer.add_tokens method")
        num_added = tokenizer.add_tokens(special_tokens)
        print(f"‚úÖ Added {num_added} tokens")
        model.resize_token_embeddings(len(tokenizer))
        vocab_size = len(tokenizer)
        print("‚úÖ Successfully added tokens via add_tokens")
    
    else:
        raise AttributeError("No suitable method found for adding tokens")
        
except Exception as e:
    print(f"‚ùå Error adding special tokens: {e}")
    print("üîÑ Falling back to text-based classification...")
    
    # Fallback: Use text-based approach instead
    special_tokens = None
    vocab_size = None
    
    print(f"\nüìù FALLBACK MODE ACTIVATED")
    print(f"   Reason: {str(e)}")
    print(f"   Will use class names directly instead of special tokens")

# Create bidirectional mapping between class names and special tokens (if successful)
if special_tokens is not None:
    class_to_token = {class_name: special_tokens[i] for i, class_name in enumerate(class_names)}
    token_to_class = {special_tokens[i]: class_name for i, class_name in enumerate(class_names)}
    
    print(f"‚úÖ Added {len(special_tokens)} special tokens")
    print(f"üìä Tokenizer vocabulary size: {vocab_size}")
    print(f"üîó Example mappings:")
    for i, (class_name, token) in enumerate(list(class_to_token.items())[:3]):
        print(f"   {class_name} ‚Üí {token}")
    print("   ...")
    
    # Log special tokens configuration to WandB
    wandb.log({
        "config/num_special_tokens": len(special_tokens),
        "config/vocab_size_after_tokens": vocab_size,
        "config/example_tokens": special_tokens[:5],
        "config/approach": "special_tokens"
    })
else:
    # Fallback: Use text-based classification with class names
    class_to_token = {class_name: class_name for class_name in class_names}  # Identity mapping
    token_to_class = {class_name: class_name for class_name in class_names}  # Identity mapping
    
    print(f"‚ö†Ô∏è Using fallback text-based classification")
    print(f"üìù Will train model to output class names directly")
    
    # Log fallback configuration to WandB
    wandb.log({
        "config/num_classes": len(class_names),
        "config/approach": "text_classification",
        "config/fallback_reason": "special_tokens_not_supported"
    })

#%%
# ========== VOCABULARY OFFSET PATCH ==========
print("\nüîß APPLYING VOCABULARY OFFSET PATCH...")
print("=" * 60)

# Import torch for tensor operations
import torch

# ÁèæÂú®„ÅÆË®≠ÂÆö„ÇíÁ¢∫Ë™ç
current_text_vocab = len(tokenizer) if hasattr(tokenizer, '__len__') else len(actual_tokenizer)
original_text_vocab = 262400  # Gemma3n„ÅÆÂÖÉ„Çµ„Ç§„Ç∫
audio_vocab_size = 128
vision_vocab_size = 128

print(f"üìä Vocabulary Status:")
print(f"   Original text vocab: {original_text_vocab}")
print(f"   Current text vocab: {current_text_vocab}")
print(f"   Audio vocab: {audio_vocab_size}")
print(f"   Vision vocab: {vision_vocab_size}")

# Ê≠£„Åó„ÅÑ„Ç™„Éï„Çª„ÉÉ„ÉàË®àÁÆó
new_text_vocab_offset = 0
new_audio_vocab_offset = current_text_vocab
new_vision_vocab_offset = current_text_vocab + audio_vocab_size
total_vocab_size = current_text_vocab + audio_vocab_size + vision_vocab_size

print(f"\nüéØ Corrected Offsets:")
print(f"   Text vocab offset: {new_text_vocab_offset} - {new_audio_vocab_offset-1}")
print(f"   Audio vocab offset: {new_audio_vocab_offset} - {new_vision_vocab_offset-1}")
print(f"   Vision vocab offset: {new_vision_vocab_offset} - {total_vocab_size-1}")

# „É¢„Éá„É´Ë®≠ÂÆö„ÇíÊõ¥Êñ∞
if hasattr(model, 'config'):
    model.config.text_vocab_offset = new_text_vocab_offset
    model.config.audio_vocab_offset = new_audio_vocab_offset
    model.config.vision_vocab_offset = new_vision_vocab_offset
    model.config.total_vocab_size = total_vocab_size
    print("‚úÖ Model config updated with corrected offsets")

# Êé®Ë´ñÈñ¢Êï∞„Çí„Éë„ÉÉ„ÉÅÔºàÂæå„Åß‰ΩøÁî®Ôºâ
def do_gemma_3n_inference_safe(messages, max_new_tokens=128):
    """ÂÆâÂÖ®„Å™Êé®Ë´ñÈñ¢Êï∞ - token IDÁØÑÂõ≤„ÉÅ„Çß„ÉÉ„ÇØ‰ªò„Åç"""
    inputs = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt",
    ).to("cuda")
    
    # Token IDÁØÑÂõ≤„ÉÅ„Çß„ÉÉ„ÇØ
    input_ids = inputs['input_ids']
    max_token_id = input_ids.max().item()
    
    if max_token_id >= total_vocab_size:
        print(f"‚ö†Ô∏è  Token ID {max_token_id} >= total_vocab_size {total_vocab_size}, clamping...")
        inputs['input_ids'] = torch.clamp(input_ids, 0, total_vocab_size - 1)
    
    try:
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=1.0, top_p=0.95, top_k=64,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )
        response_tokens = outputs[0][inputs['input_ids'].shape[1]:]
        response_text = tokenizer.decode(response_tokens, skip_special_tokens=True)
        return response_text
    except Exception as e:
        print(f"‚ùå Generation error: {e}")
        return f"[Error: {str(e)}]"

print("‚úÖ Patched inference function created")

# WandB„Å´„Ç™„Éï„Çª„ÉÉ„Éà‰øÆÊ≠£ÊÉÖÂ†±„Çí„É≠„Ç∞
wandb.log({
    "vocab_patch/original_text_vocab": original_text_vocab,
    "vocab_patch/current_text_vocab": current_text_vocab,
    "vocab_patch/new_audio_offset": new_audio_vocab_offset,
    "vocab_patch/new_vision_offset": new_vision_vocab_offset,
    "vocab_patch/total_vocab_size": total_vocab_size,
    "vocab_patch/patch_applied": True
})
print("üìä Vocabulary offset patch info logged to WandB")
print("=" * 60)

#%%
# Convert to conversation format with adaptive approach
# Create structured instruction based on approach used
if special_tokens is not None:
    # Special token approach
    token_list_str = "\n".join([f"- {class_names[i]} ‚Üí {special_tokens[i]}" for i in range(len(class_names))])
    
    instruction = f"""You are an expert plant disease classifier. 
Analyze the image and respond with ONLY the corresponding special token:

{token_list_str}

IMPORTANT: 
- Return exactly one special token from the mappings above
- Use the exact token format: <PLANT_CLASS_N>
- Do not add any explanation or additional text
- Example response: <PLANT_CLASS_5>"""
    
    print(f"üìù Instruction created with {len(class_names)} classes mapping to special tokens")

else:
    # Text-based approach (fallback)
    class_list_str = "\n".join([f"- {cls}" for cls in class_names])
    
    instruction = f"""You are an expert plant disease classifier. 
Analyze the image and respond with ONLY the exact class name from this list:

{class_list_str}

IMPORTANT: 
- Return exactly one class name from the list above
- Use the exact format shown (including underscores if present)
- Do not add any explanation or additional text
- Example response: "Apple_Black_rot"""
    
    print(f"üìù Instruction created with {len(class_names)} classes (text-based fallback)")

def convert_to_conversation(sample):
    try:
        # Load image as PIL Image object instead of path string
        image = Image.open(sample["path"])
        # Convert to RGB if needed (handles RGBA, grayscale, etc.)
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Get the special token for this class
        special_token = class_to_token[sample["class"]]
        
        conversation = [
            { "role": "user",
              "content" : [
                {"type" : "text",  "text"  : instruction},
                {"type" : "image", "image" : image} ]
            },
            { "role" : "assistant",
              "content" : [
                {"type" : "text",  "text"  : special_token} ]
            },
        ]
        return { "messages" : conversation }
    except Exception as e:
        print(f"‚ùå Error loading image {sample['path']}: {e}")
        return None

#%%
# Convert train and validation datasets to conversation format separately
train_converted_dataset = [convert_to_conversation(sample) for sample in train_dataset]
train_converted_dataset = [item for item in train_converted_dataset if item is not None]
print(f"‚úÖ Converted {len(train_converted_dataset)} training samples to conversation format")

val_converted_dataset = [convert_to_conversation(sample) for sample in val_dataset]
val_converted_dataset = [item for item in val_converted_dataset if item is not None]
print(f"‚úÖ Converted {len(val_converted_dataset)} validation samples to conversation format")

# Additional validation dataset size control for SFT
max_val_samples_for_sft = 200  # Limit validation samples during SFT training
# Note: You can adjust this value based on your needs:
# - Small (50-100): Very fast evaluation, less comprehensive
# - Medium (200-500): Balanced speed and coverage  
# - Large (1000+): More comprehensive but slower
if len(val_converted_dataset) > max_val_samples_for_sft:
    val_converted_dataset = random.sample(val_converted_dataset, max_val_samples_for_sft)
    print(f"‚öôÔ∏è Reduced validation dataset to {len(val_converted_dataset)} samples for SFT efficiency")

# Show sample
print("\nüìã Sample training conversation:")
print(train_converted_dataset[0])

print(f"\nüìä Final dataset split:")
print(f"   Training samples: {len(train_converted_dataset)}")
print(f"   Validation samples: {len(val_converted_dataset)} (optimized for SFT)")
print(f"   Validation ratio: {len(val_converted_dataset)/len(train_converted_dataset):.2%}")

#%%
# Vision Fine-tuning (exactly as in CLAUDE.md)
print("\nüöÄ Starting Vision Fine-tuning...")

# Add PEFT configuration for quantized model
model = FastVisionModel.get_peft_model(
    model,
    finetune_vision_layers = True,
    finetune_language_layers = True,
    finetune_attention_modules = True,
    finetune_mlp_modules = True,
    r = 32,  # LoRA rank
    lora_alpha = 64,
    lora_dropout = 0,  # Changed from 0.1 to 0 for Unsloth compatibility
    bias = "none",
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "vision_embed"],
    random_state = 3407,
    use_gradient_checkpointing = "unsloth",
)

# Enable for training
FastVisionModel.for_training(model)

# Note: Accuracy metrics removed - using special token prediction instead

# Create trainer with validation dataset and evaluation settings  
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    data_collator = UnslothVisionDataCollator(model, tokenizer), # Must use!
    train_dataset = train_converted_dataset,  # Use converted training data
    eval_dataset = val_converted_dataset,     # Use converted validation data
    # Note: compute_metrics removed - focusing on loss monitoring only
    args = SFTConfig(
        per_device_train_batch_size = 1,  # Minimal batch size for debugging
        per_device_eval_batch_size = 2,   # Very small validation batch for safety
        gradient_accumulation_steps = 4,  # Reduced to avoid memory issues
        warmup_steps = 2,
        max_steps = 5,  # Very short for initial testing
        learning_rate = 1e-4,
        fp16 = not is_bf16_supported(),
        bf16 = is_bf16_supported(),
        logging_steps = 1,               # Log every step for detailed monitoring
        logging_strategy = "steps",      # Log based on steps
        logging_first_step = True,       # Log the first step
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "./plant_vision_outputs",
        
        # Enhanced WandB reporting for detailed metrics
        report_to = "wandb",
        run_name = "gemma3n-plant-vision-ft",
        logging_nan_inf_filter = True,   # Filter out NaN/Inf values from logs
        
        # Enhanced loss and metric logging
        log_on_each_node = True,         # Log on each GPU node
        logging_dir = "./plant_vision_outputs/logs",  # Directory for detailed logs
        
        dataloader_pin_memory = False,   # Disable pin memory to avoid conflicts
        dataloader_num_workers = 0,     # Disable multiprocessing to avoid issues
        
        # Evaluation settings for detailed monitoring
        eval_strategy = "steps",         # Enable evaluation
        eval_steps = 10,                 # Evaluate more frequently (every 10 steps)
        eval_delay = 0,                  # Start evaluation immediately
        
        # Save and logging settings
        save_strategy = "steps",         # Save based on steps
        save_steps = 20,                 # Must be multiple of eval_steps (10)
        save_total_limit = 3,            # Keep only last 3 checkpoints
        load_best_model_at_end = True,   # Load best model based on validation loss
        metric_for_best_model = "eval_loss",  # Use validation loss for best model
        greater_is_better = False,       # Lower loss is better
        
        # Additional logging settings
        log_level = "info",              # Set log level to info
        disable_tqdm = False,            # Keep progress bars enabled
        dataloader_drop_last = False,    # Don't drop last incomplete batch

        # You MUST put the below items for vision finetuning (from CLAUDE.md):
        remove_unused_columns = False,
        dataset_text_field = "",
        dataset_kwargs = {"skip_prepare_dataset": True},
        dataset_num_proc = 1,
        max_seq_length = 2048,
    ),
)

# Start training
print("üìà Training started...")
trainer.train()

print("üéâ Training completed!")
print(f"üíæ Model saved to: ./plant_vision_outputs")

if special_tokens is not None:
    print(f"üè∑Ô∏è Model trained to predict special tokens: {special_tokens[:3]}... (total: {len(special_tokens)})")
    model_type = "special_token_classification"
    log_data = {
        "training_status": "completed",
        "model_type": model_type,
        "num_special_tokens": len(special_tokens)
    }
else:
    print(f"üìù Model trained for text-based classification with {len(class_names)} classes")
    model_type = "text_classification"
    log_data = {
        "training_status": "completed", 
        "model_type": model_type,
        "num_classes": len(class_names)
    }

# Log training completion to WandB
wandb.log(log_data)

#%%
# Test the model
print("\nüß™ Testing model...")

# Get a test sample and load as PIL Image (use validation dataset)
test_sample = val_dataset[0]
test_image = Image.open(test_sample["path"])
if test_image.mode != 'RGB':
    test_image = test_image.convert('RGB')

test_messages = [{
    "role": "user",
    "content": [
        {"type": "text", "text": instruction},
        {"type": "image", "image": test_image}
    ]
}]

# Generate response using safe inference function
response = do_gemma_3n_inference_safe(test_messages, max_new_tokens=50)

print(f"‚úÖ Test completed!")
print(f"\nüñºÔ∏è Test Image: {os.path.basename(test_sample['path'])}")
print("=" * 50)

# Display the test image
display(test_image)

print(f"üè∑Ô∏è Actual class: {test_sample['class']}")
if special_tokens is not None:
    print(f"üîó Expected token: {class_to_token[test_sample['class']]}")
else:
    print(f"üìù Expected response: {test_sample['class']}")
print(f"ü§ñ Model response: {response}")

# Utility functions for detailed logging
def log_prediction_result(step_name: str, actual_label: str, predicted_label: str, 
                         response_text: str, confidence: float, is_correct: bool,
                         image=None, step_number: int = None):
    """Log detailed prediction result to WandB"""
    log_data = {
        f"{step_name}/actual_label": actual_label,
        f"{step_name}/predicted_label": predicted_label or "None",
        f"{step_name}/response_text": response_text,
        f"{step_name}/confidence": confidence,
        f"{step_name}/is_correct": is_correct,
    }
    
    if step_number is not None:
        log_data[f"{step_name}/step_number"] = step_number
    
    if image is not None:
        log_data[f"{step_name}/test_image"] = wandb.Image(image, caption=f"Actual: {actual_label}")
    
    wandb.log(log_data)

# Import statement moved above with other imports

def update_class_statistics(class_stats: dict, actual_class: str, predicted_class: str, is_correct: bool):
    """Update running class-wise statistics"""
    if actual_class not in class_stats:
        class_stats[actual_class] = {"total": 0, "correct": 0, "predictions": []}
    
    class_stats[actual_class]["total"] += 1
    if is_correct:
        class_stats[actual_class]["correct"] += 1
    class_stats[actual_class]["predictions"].append(predicted_class or "None")
    
    return class_stats

# Enhanced prediction extraction for adaptive approach
def extract_predicted_label(response_text: str, available_classes: list) -> tuple:
    """Extract predicted class from response (special token or text-based)
    
    Args:
        response_text: Model response text
        available_classes: List of valid class names
        
    Returns:
        tuple: (predicted_class, confidence_score)
    """
    import re
    from difflib import SequenceMatcher
    
    response_clean = response_text.strip()
    
    if special_tokens is not None:
        # Special token approach
        # Step 1: Look for exact special token pattern
        token_pattern = r'<PLANT_CLASS_(\d+)>'
        matches = re.findall(token_pattern, response_clean)
        
        if matches:
            try:
                # Extract the class index from the first match
                class_index = int(matches[0])
                
                # Check if index is valid
                if 0 <= class_index < len(class_names):
                    predicted_class = class_names[class_index]
                    return predicted_class, 1.0  # High confidence for exact token match
                else:
                    print(f"‚ö†Ô∏è Invalid class index {class_index} (max: {len(class_names)-1})")
            except ValueError:
                print(f"‚ö†Ô∏è Could not parse class index from token")
        
        # Step 2: Look for any special token in the response (more permissive)
        general_token_pattern = r'<PLANT_CLASS_\d+>'
        token_matches = re.findall(general_token_pattern, response_clean)
        
        for token in token_matches:
            if token in token_to_class:
                predicted_class = token_to_class[token]
                return predicted_class, 0.95  # High confidence for valid token
    
    # Step 3: Text-based matching (used for fallback or if no tokens found)
    # Exact match (highest confidence)
    for class_name in available_classes:
        if class_name == response_clean:
            return class_name, 0.9 if special_tokens is None else 0.7
    
    # Case-insensitive exact match
    response_lower = response_clean.lower()
    for class_name in available_classes:
        if class_name.lower() == response_lower:
            return class_name, 0.85 if special_tokens is None else 0.65
    
    # Substring match
    for class_name in available_classes:
        if class_name in response_clean:
            return class_name, 0.8 if special_tokens is None else 0.6
        if class_name.lower() in response_lower:
            return class_name, 0.75 if special_tokens is None else 0.55
    
    # Fuzzy matching for robustness
    best_match = None
    best_score = 0.0
    
    for class_name in available_classes:
        similarity = SequenceMatcher(None, response_lower, class_name.lower()).ratio()
        if similarity > best_score and similarity > 0.6:
            best_score = similarity
            best_match = class_name
    
    if best_match:
        confidence = best_score * (0.7 if special_tokens is None else 0.5)
        return best_match, confidence
    
    # Step 4: No valid prediction found
    approach = "special token" if special_tokens is not None else "text-based"
    print(f"‚ö†Ô∏è No valid {approach} prediction found in response: '{response_clean}'")
    return available_classes[0] if available_classes else None, 0.1

class_names = train_dataset_folder.classes
predicted_class, confidence = extract_predicted_label(response, class_names)

print(f"üéØ Prediction confidence: {confidence:.2%}")

if predicted_class:
    print(f"üîç Extracted predicted class: {predicted_class}")
    
    # Find examples of the predicted class (from combined dataset)
    combined_dataset = train_dataset + val_dataset
    predicted_class_samples = [item for item in combined_dataset if item['class'] == predicted_class]
    
    if predicted_class_samples:
        print(f"\nüìö Reference images for '{predicted_class}' (showing 3 examples):")
        print("-" * 50)
        
        # Show up to 3 reference images
        reference_samples = random.sample(predicted_class_samples, min(3, len(predicted_class_samples)))
        
        for i, ref_sample in enumerate(reference_samples, 1):
            ref_image = Image.open(ref_sample["path"])
            if ref_image.mode != 'RGB':
                ref_image = ref_image.convert('RGB')
            
            print(f"Reference {i}: {os.path.basename(ref_sample['path'])}")
            display(ref_image)
            print()
    else:
        print(f"‚ùå No reference images found for '{predicted_class}'")
else:
    print("‚ùì Could not extract a clear class prediction from the response")

# Check if prediction is correct (use exact matching)
is_correct = (predicted_class == test_sample['class']) if predicted_class else False
is_partial_match = test_sample['class'].lower() in response.lower()

result_emoji = "‚úÖ" if is_correct else ("üü°" if is_partial_match else "‚ùå")
accuracy_status = "Exact Match" if is_correct else ("Partial Match" if is_partial_match else "Incorrect")
print(f"{result_emoji} Prediction: {accuracy_status}")

# Log detailed single test result
log_prediction_result(
    step_name="single_test",
    actual_label=test_sample['class'],
    predicted_label=predicted_class,
    response_text=response,
    confidence=confidence,
    is_correct=is_correct,
    image=test_image,
    step_number=1
)

# Log to WandB
wandb_log_data = {
    "single_test/test_image": wandb.Image(test_image, caption=f"Test: {test_sample['class']}"),
    "single_test/actual_class": test_sample['class'],
    "single_test/predicted_response": response,
    "single_test/is_correct": is_correct
}

if predicted_class:
    wandb_log_data["single_test/extracted_class"] = predicted_class
    wandb_log_data["single_test/prediction_confidence"] = confidence
    
    # Log reference images if available
    if predicted_class_samples:
        for j, ref_sample in enumerate(reference_samples[:2], 1):  # Log first 2 reference images
            ref_img = Image.open(ref_sample["path"])
            if ref_img.mode != 'RGB':
                ref_img = ref_img.convert('RGB')
            wandb_log_data[f"single_test/reference_{j}"] = wandb.Image(
                ref_img, 
                caption=f"Ref {j}: {predicted_class}"
            )

wandb.log(wandb_log_data)
wandb.finish()