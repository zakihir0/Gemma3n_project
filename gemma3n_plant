#%%
# Vision Fine-tuning for plnat Classification - CLAUDE.md Based

!pip uninstall -y pygobject gradient gradient-utils
!pip install --no-cache-dir --upgrade packaging==24.2
!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo
!pip install --upgrade bitsandbytes
!pip install triton==3.2.0
!pip install pip3-autoremove
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
!pip install unsloth
!pip install faiss-cpu
!pip install sentence-transformers
!pip install wikipedia
!pip install --no-deps git+https://github.com/huggingface/transformers.git
!pip install --no-deps --upgrade timm
!pip uninstall -y deepspeed
!pip install deepspeed==0.14.4
!pip install wandb==0.17.9

#%%
import os
import random
import pandas as pd
import json
from typing import List
from collections import defaultdict
from torchvision import datasets
from unsloth import FastModel, is_bf16_supported, FastVisionModel
from unsloth.trainer import UnslothVisionDataCollator
from trl import SFTTrainer
from transformers import TrainingArguments, Trainer
import wandb
from PIL import Image
from IPython.display import display, Image as IPImage

os.environ["CUDA_VISIBLE_DEVICES"] = "0, 1"
os.environ["WANDB_API_KEY"] = "abd0ce2837eca439d66f6c603136603c1729cd3e"

#%%
# Initialize WandB with environment variable login
wandb.login(key=os.getenv("WANDB_API_KEY"))

wandb.init(
    project="gemma3n-plant-classification",
    name="vision-finetuning-experiment",
    config={
        "model_name": "unsloth/gemma-3n-E2B-it",
        "max_seq_length": 1024,
        "load_in_4bit": True,
        "max_files_per_class_train": 10000,
        "max_files_per_class_val": 50,
        "max_val_samples_for_sft": 200,
        "lora_rank": 32,
        "lora_alpha": 64,
        "learning_rate": 1e-4,
        "max_steps": 300,
        "eval_steps": 30,
        "per_device_train_batch_size": 8,
        "per_device_eval_batch_size": 8,
        "gradient_accumulation_steps": 8,
    }
)

# Load the model (exactly as in CLAUDE.md)
model, tokenizer = FastModel.from_pretrained(
    model_name = "unsloth/gemma-3n-E2B-it",
    dtype = None,
    max_seq_length = 1024,
    load_in_4bit = True,
    full_finetuning = False,
)

#%%
# Data loading using existing train/valid split
base_dir = '/notebooks/kaggle/input/new_plant_diseases/2/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)'
train_dir = os.path.join(base_dir, 'train')
valid_dir = os.path.join(base_dir, 'valid')

# Dataset size configuration
max_files_per_class_train = 10000  # Training data limit
max_files_per_class_val = 50       # Validation data limit (small for faster evaluation)

print(f"üìä Dataset configuration:")
print(f"   Training: max {max_files_per_class_train} files per class")
print(f"   Validation: max {max_files_per_class_val} files per class")

def load_dataset_from_dir(root_dir, max_files_per_class=None):
    """Load dataset from directory with optional file limit per class"""
    classes = []
    paths = []
    class_file_counts = {}
    
    for dirname, _, filenames in os.walk(root_dir):
        class_name = dirname.split('/')[-1]
        if class_name == os.path.basename(root_dir):  # Skip root directory
            continue
            
        if class_name not in class_file_counts:
            class_file_counts[class_name] = 0
        
        for filename in filenames:
            if max_files_per_class is None or class_file_counts[class_name] < max_files_per_class:
                if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):
                    full_path = os.path.join(dirname, filename)
                    if os.path.exists(full_path):
                        classes.append(class_name)
                        paths.append(full_path)
                        class_file_counts[class_name] += 1
    
    return classes, paths, class_file_counts

# Load train and validation datasets separately with different limits
train_classes, train_paths, train_counts = load_dataset_from_dir(train_dir, max_files_per_class_train)
val_classes, val_paths, val_counts = load_dataset_from_dir(valid_dir, max_files_per_class_val)

# Get unified class names from ImageFolder for consistency
train_dataset_folder = datasets.ImageFolder(root=train_dir)
class_names = train_dataset_folder.classes
print(f"Class names: {class_names}")
print(f"Number of classes: {len(class_names)}")

# Create mapping
N = list(range(len(class_names)))
normal_mapping = dict(zip(class_names, N))
reverse_mapping = dict(zip(N, class_names))

# Create train dataset
train_dataset = []
for path, class_name in zip(train_paths, train_classes):
    train_dataset.append({"path": path, "class": class_name})

# Create validation dataset
val_dataset = []
for path, class_name in zip(val_paths, val_classes):
    val_dataset.append({"path": path, "class": class_name})

print(f"Training samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")
print(f"Train class distribution: {train_counts}")
print(f"Val class distribution: {val_counts}")

# „ÉÜ„Ç≠„Çπ„ÉàÊåáÁ§∫„ÅØ‰∏çË¶Å - VisionÂàÜÈ°û„Åß„ÅØÁîªÂÉè„ÅÆ„Åø„Çí‰ΩøÁî®

#%%
# VisionÂàÜÈ°ûÁî®„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„ÉàÂá¶ÁêÜ
def process_vision_dataset(dataset, dataset_name, class_names):
    """VisionÂàÜÈ°ûÁî®„Å´„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÂá¶ÁêÜÔºàÁîªÂÉè„Å®„ÇØ„É©„ÇπID„ÅÆ„Éö„Ç¢Ôºâ"""
    processed_dataset = []
    normal_mapping = {cls: idx for idx, cls in enumerate(class_names)}
    
    for sample in dataset:
        try:
            # ÁîªÂÉè„ÇíË™≠„ÅøËæº„Åø
            image = Image.open(sample["path"])
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # „ÇØ„É©„ÇπÂêç„ÇíID„Å´Â§âÊèõ
            class_id = normal_mapping[sample["class"]]
            
            processed_dataset.append({
                "image": image,
                "labels": class_id  # „ÇØ„É©„ÇπID (int)
            })
            
        except Exception as e:
            print(f"‚ùå Error processing {dataset_name} sample {sample['path']}: {e}")
    
    return processed_dataset

# VisionÂàÜÈ°ûÁî®„Éá„Éº„Çø„Çª„ÉÉ„Éà‰ΩúÊàê
train_vision_dataset = process_vision_dataset(train_dataset, "training", class_names)
print(f"‚úÖ Converted {len(train_vision_dataset)} training samples for vision classification")

val_vision_dataset = process_vision_dataset(val_dataset, "validation", class_names)
print(f"‚úÖ Converted {len(val_vision_dataset)} validation samples for vision classification")

# ÂæìÊù•„ÅÆconversationÂΩ¢Âºè„ÅÆÂá¶ÁêÜ„ÅØÂâäÈô§ - VisionÂàÜÈ°û„ÅÆ„Åø„Çí‰ΩøÁî®

# Show sample from vision dataset
print("\nüìã Sample vision dataset entry:")
print(f"Image path: {train_vision_dataset[0]['image']}")
print(f"Label: {train_vision_dataset[0]['labels']} (class: {class_names[train_vision_dataset[0]['labels']]})")

print(f"\nüìä Final dataset split:")
print(f"   Training samples: {len(train_vision_dataset)}")
print(f"   Validation samples: {len(val_vision_dataset)}")
print(f"   Validation ratio: {len(val_vision_dataset)/len(train_vision_dataset):.2%}")

#%%
# Vision Fine-tuning - Linear head only
print("\nüöÄ Starting Vision Fine-tuning (Linear head only)...")

import torch
import torch.nn.functional as F

import torch.nn as nn
from transformers import Trainer
from torch.utils.data import DataLoader
from transformers.modeling_outputs import SequenceClassifierOutput
import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from transformers import TrainerCallback


def ConvBlock(in_channels, out_channels, pool=False):
    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
             nn.BatchNorm2d(out_channels),
             nn.ReLU(inplace=True)]
    if pool:
        layers.append(nn.MaxPool2d(4))
    return nn.Sequential(*layers)

class ClearMemoryCallback(TrainerCallback):
    """Callback that frees CUDA cache every `clear_every_n_steps` training steps."""
    def __init__(self, clear_every_n_steps: int = 50):
        self.clear_every_n_steps = clear_every_n_steps

    def on_step_end(self, args, state, control, **kwargs):
        if state.global_step and state.global_step % self.clear_every_n_steps == 0:
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
        return control

class VisionCNNClassifier(nn.Module):
    """Vision TowerÁâπÂæ¥Èáè ‚Üí CNNÂàÜÈ°ûÂô®„ÅÆ„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£"""
    def __init__(self, vision_model, num_classes, hidden_dim):
        super().__init__()
        self.vision_model = vision_model
        self.hidden_dim = hidden_dim
        
        # Vision Tower„ÅÆÁâπÂæ¥Èáè„Çífreeze„Åô„Çã
        for param in self.vision_model.parameters():
            param.requires_grad = False
        
        # CNNÂàÜÈ°ûÂô® (hidden_dim„Çíin_channels„Å®„Åó„Å¶‰ΩøÁî®)
        self.conv1 = ConvBlock(hidden_dim, 64)
        self.conv2 = ConvBlock(64, 128, pool=True) 
        self.res1 = nn.Sequential(ConvBlock(128, 128), ConvBlock(128, 128))
        
        self.conv3 = ConvBlock(128, 256, pool=True) 
        self.conv4 = ConvBlock(256, 512, pool=True)
        
        self.res2 = nn.Sequential(ConvBlock(512, 512), ConvBlock(512, 512))
        # 64x64 ‚Üí conv2(16x16) ‚Üí conv3(4x4) ‚Üí conv4(1x1) „Å´„Å™„Çã„Åü„ÇÅ
        # 1x1„ÉÜ„É≥„ÇΩ„É´„Å™„ÅÆ„ÅßMaxPool2d„ÅØ‰∏çË¶Å„ÄÅÁõ¥Êé•Flatten„Åô„Çã
        self.classifier = nn.Sequential(nn.Flatten(),
                                       nn.Linear(512, num_classes))
    
    def extract_vision_features(self, pixel_values):
        """Vision Tower„Åã„ÇâÁâπÂæ¥Èáè„ÇíÊäΩÂá∫„Åó„ÄÅCNNÁî®„ÅÆÂΩ¢Áä∂„Å´Â§âÊèõ"""
        with torch.no_grad():  # Vision Tower„ÅØfreeze„Åï„Çå„Å¶„ÅÑ„Çã„ÅÆ„Åßgradient‰∏çË¶Å
            vision_output = self.vision_model(pixel_values)
            vision_features = vision_output.last_hidden_state
        
        # vision_features„ÅÆÂΩ¢Áä∂„ÇíÁ¢∫Ë™ç„Åó„Å¶„ÄÅCNN„ÅåÊúüÂæÖ„Åô„ÇãÂΩ¢Áä∂„Å´Â§âÊèõ
        if len(vision_features.shape) == 3:
            # (B, seq_len, hidden_dim) -> spatial dimension„Å´Â§âÊèõ
            B, seq_len, hidden_dim = vision_features.shape
            # seq_len„ÅåÂÆåÂÖ®Âπ≥ÊñπÊï∞„Åß„ÅÇ„Çã„Åì„Å®„Çí‰ªÆÂÆö„Åó„Å¶„ÄÅspatial dimension„Å´ÂÜçÂΩ¢Êàê
            spatial_size = int(seq_len ** 0.5)
            if spatial_size * spatial_size == seq_len:
                # (B, hidden_dim, H, W) ÂΩ¢Âºè„Å´Â§âÊèõ
                vision_features = vision_features.transpose(1, 2).reshape(B, hidden_dim, spatial_size, spatial_size)
            else:
                # ÂÆåÂÖ®Âπ≥ÊñπÊï∞„Åß„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅÈÅ©ÂΩì„Å™„Çµ„Ç§„Ç∫„Å´Ë™øÊï¥
                spatial_size = int(seq_len ** 0.5) + 1
                pad_size = spatial_size * spatial_size - seq_len
                if pad_size > 0:
                    padding = torch.zeros(B, pad_size, hidden_dim, device=vision_features.device)
                    vision_features = torch.cat([vision_features, padding], dim=1)
                vision_features = vision_features.transpose(1, 2).reshape(B, hidden_dim, spatial_size, spatial_size)
        elif len(vision_features.shape) == 4:
            # Êó¢„Å´ (B, C, H, W) ÂΩ¢Âºè
            pass
        else:
            raise ValueError(f"Unexpected vision_features shape: {vision_features.shape}")
        
        # AdaptiveAvgPool2d„ÅßCNN„Å´ÈÅ©„Åó„Åü„Çµ„Ç§„Ç∫„Å´„É™„Çµ„Ç§„Ç∫Ôºà64x64„Å´Ë®≠ÂÆöÔºâ
        adaptive_pool = nn.AdaptiveAvgPool2d((64, 64))
        vision_features = adaptive_pool(vision_features)
        
        return vision_features
    
    def forward(self, pixel_values, **kwargs):
        """ImageClassificationBase„Å®‰∫íÊèõÊÄß„ÅÆ„ÅÇ„Çãforward method"""
        # Vision Tower„Åã„ÇâÁâπÂæ¥Èáè„ÇíÊäΩÂá∫
        vision_features = self.extract_vision_features(pixel_values)
        
        # CNN layers through the network
        out = self.conv1(vision_features)
        out = self.conv2(out)
        out = self.res1(out) + out
        out = self.conv3(out)
        out = self.conv4(out)
        out = self.res2(out) + out
        out = self.classifier(out)
        
        return out

# ID/LabelÂ§âÊèõ„É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£Èñ¢Êï∞
def id_to_label(class_id, class_names_list=None):
    """„ÇØ„É©„ÇπID„Çí„ÇØ„É©„ÇπÂêç„Å´Â§âÊèõ
    
    Args:
        class_id (int): „ÇØ„É©„ÇπID (0 ~ len(class_names)-1)
        class_names_list (list, optional): „ÇØ„É©„ÇπÂêç„É™„Çπ„Éà„ÄÇNone„ÅÆÂ†¥Âêà„ÅØ„Ç∞„É≠„Éº„Éê„É´„ÅÆclass_names„Çí‰ΩøÁî®
    
    Returns:
        str: „ÇØ„É©„ÇπÂêç
    """
    if class_names_list is None:
        class_names_list = class_names
    return class_names_list[class_id]

def ids_to_labels(class_ids, class_names_list=None):
    """„ÇØ„É©„ÇπID„É™„Çπ„Éà/ÈÖçÂàó„Çí„ÇØ„É©„ÇπÂêç„É™„Çπ„Éà„Å´Â§âÊèõ
    
    Args:
        class_ids (list/array): „ÇØ„É©„ÇπID„ÅÆ„É™„Çπ„Éà„Åæ„Åü„ÅØÈÖçÂàó
        class_names_list (list, optional): „ÇØ„É©„ÇπÂêç„É™„Çπ„Éà„ÄÇNone„ÅÆÂ†¥Âêà„ÅØ„Ç∞„É≠„Éº„Éê„É´„ÅÆclass_names„Çí‰ΩøÁî®
    
    Returns:
        list: „ÇØ„É©„ÇπÂêç„ÅÆ„É™„Çπ„Éà
    """
    if class_names_list is None:
        class_names_list = class_names
    return [class_names_list[id] for id in class_ids]

def get_top_k_predictions(logits, k=3, class_names_list=None):
    """logits„Åã„ÇâTop-K‰∫àÊ∏¨„Å®„Åù„ÅÆÁ¢∫‰ø°Â∫¶„ÇíËøî„Åô
    
    Args:
        logits (torch.Tensor): „É¢„Éá„É´„ÅÆÂá∫Âäõlogits (batch_size, num_classes)
        k (int): ‰∏ä‰Ωç‰ΩïÂÄã„Åæ„ÅßÂèñÂæó„Åô„Çã„Åã
        class_names_list (list, optional): „ÇØ„É©„ÇπÂêç„É™„Çπ„Éà„ÄÇNone„ÅÆÂ†¥Âêà„ÅØ„Ç∞„É≠„Éº„Éê„É´„ÅÆclass_names„Çí‰ΩøÁî®
    
    Returns:
        list: „Éê„ÉÉ„ÉÅÂÜÖÂêÑ„Çµ„É≥„Éó„É´„ÅÆTop-K‰∫àÊ∏¨ÁµêÊûú
              [{'class_id': int, 'class_name': str, 'confidence': float}, ...]
    """
    import torch
    
    if class_names_list is None:
        class_names_list = class_names
    
    # logits„Åå1Ê¨°ÂÖÉ„ÅÆÂ†¥Âêà(Âçò‰∏Ä„Çµ„É≥„Éó„É´)„ÅØ2Ê¨°ÂÖÉ„Å´Â§âÊèõ
    if len(logits.shape) == 1:
        logits = logits.unsqueeze(0)
    
    # Á¢∫Áéá„Å´Â§âÊèõ„Åó„Å¶Top-KÂèñÂæó
    probs = torch.softmax(logits, dim=-1)
    top_k_probs, top_k_ids = torch.topk(probs, k, dim=-1)
    
    results = []
    for i in range(len(top_k_ids)):
        predictions = []
        for j in range(k):
            class_id = top_k_ids[i][j].item()
            confidence = top_k_probs[i][j].item()
            class_name = class_names_list[class_id]
            predictions.append({
                'class_id': class_id,
                'class_name': class_name,
                'confidence': confidence
            })
        results.append(predictions)
    
    return results

# „Çπ„Éû„Éº„ÉàÈÅ©ÂøúÁöÑ‰∫àÊ∏¨„Ç∑„Çπ„ÉÜ„É†
def do_gemma_3n_inference(messages, max_new_tokens=128):
    """VisionÁîüÊàêÊ©üËÉΩ - CLAUDE.md„Åã„ÇâÂæ©Ê¥ª"""
    inputs = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt",
    ).to("cuda")
    
    # Â∏∏„Å´Âá∫Âäõ„Çí„Ç≠„É£„Éó„ÉÅ„É£„Åó„Å¶Ëøî„Åô
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=1.0, top_p=0.95, top_k=64,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
    )
    # Êñ∞„Åó„ÅèÁîüÊàê„Åï„Çå„Åü„Éà„Éº„ÇØ„É≥„ÅÆ„Åø„ÇíÊäΩÂá∫
    response_tokens = outputs[0][inputs['input_ids'].shape[1]:]
    response_text = tokenizer.decode(response_tokens, skip_special_tokens=True)
    return response_text

def check_if_plant_disease_image(image):
    """VisionÁîüÊàêÊ©üËÉΩ„ÅßÊ§çÁâ©ÁóÖÊ∞óÈñ¢ÈÄ£ÁîªÂÉè„Åã„ÇíÂà§ÂÆö"""
    messages = [
        {"role": "user", 
         "content": [
             {"type": "text", "text": "Is this image related to plant diseases or plant health issues? Answer only 'YES' or 'NO' and briefly explain why."},
             {"type": "image", "image": image}
         ]}
    ]
    
    # VisionÁîüÊàê„ÅßÂà§ÂÆö
    response = do_gemma_3n_inference(messages, max_new_tokens=50)
    
    # 'YES'„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Çã„Åã„Çí„ÉÅ„Çß„ÉÉ„ÇØ
    is_plant_disease = 'YES' in response.upper()
    
    return is_plant_disease, response

def generate_detailed_description(image):
    """‰∏ÄËà¨ÁîªÂÉè„ÅÆË©≥Á¥∞Ë™¨ÊòéÁîüÊàê"""
    messages = [
        {"role": "user", 
         "content": [
             {"type": "text", "text": "Describe accurately and in detail what you see in this image."},
             {"type": "image", "image": image}
         ]}
    ]
    
    return do_gemma_3n_inference(messages, max_new_tokens=150)

def smart_predict_image(image):
    """
    „Çπ„Éû„Éº„Éà‰∫àÊ∏¨: Classifier ‚Üí ‰ø°È†ºÂ∫¶„ÉÅ„Çß„ÉÉ„ÇØ ‚Üí ÂøÖË¶ÅÊôÇVisionÂà§ÂÆö
    
    Args:
        image: PIL Image
    
    Returns:
        dict: ‰∫àÊ∏¨ÁµêÊûúËæûÊõ∏
    """
    # „Éá„Éº„Çø„Ç≥„É¨„Éº„Çø„Éº„Çí‰ΩøÁî®„Åó„Å¶pixel_values„ÇíÊ∫ñÂÇô
    test_sample = {"image": image, "labels": 0}  # „ÉÄ„Éü„Éº„É©„Éô„É´
    batch = vision_trainer.data_collator([test_sample])
    pixel_values = batch["pixel_values"].to("cuda")
    
    # Step 1: Classifier„Åß‰∫àÊ∏¨
    with torch.no_grad():
        logits = vision_classifier(pixel_values=pixel_values)
        top_predictions = get_top_k_predictions_local(logits, k=3)
        max_confidence = top_predictions[0][0]['confidence']
    
    print(f"üîç Classifier‰∫àÊ∏¨: {top_predictions[0][0]['class_name']} (‰ø°È†ºÂ∫¶: {max_confidence:.1%})")
    
    # Step 2: ‰ø°È†ºÂ∫¶„ÉÅ„Çß„ÉÉ„ÇØ
    if max_confidence >= 0.7:  # È´ò‰ø°È†ºÂ∫¶
        print("‚úÖ È´ò‰ø°È†ºÂ∫¶„ÅÆ„Åü„ÇÅ„ÄÅClassifierÁµêÊûú„ÇíÊé°Áî®")
        return {
            'prediction': top_predictions[0][0]['class_name'],
            'confidence': max_confidence,
            'method': 'classifier',
            'reason': f'È´ò‰ø°È†ºÂ∫¶ ({max_confidence:.1%})',
            'top_alternatives': top_predictions[0][:3]
        }
    else:
        # Step 3: VisionÁîüÊàê„ÅßÊ§çÁâ©ÁóÖÊ∞óÈñ¢ÈÄ£„Åã„ÇíÂà§ÂÆö
        print("ü§î ‰ø°È†ºÂ∫¶„Åå‰Ωé„ÅÑ„Åü„ÇÅ„ÄÅVisionÁîüÊàê„ÅßÁîªÂÉèÂÜÖÂÆπ„ÇíÁ¢∫Ë™ç‰∏≠...")
        is_plant_disease, vision_response = check_if_plant_disease_image(image)
        print(f"üß† VisionÂà§ÂÆö: {'Ê§çÁâ©ÁóÖÊ∞óÈñ¢ÈÄ£' if is_plant_disease else 'Ê§çÁâ©ÁóÖÊ∞ó‰ª•Â§ñ'}")
        print(f"üìù VisionÂøúÁ≠î: {vision_response}")
        
        if is_plant_disease:
            # Ê§çÁâ©ÁóÖÊ∞óÈñ¢ÈÄ£ ‚Üí Classifier„ÅÆÁµêÊûú„Çí‰ΩøÁî®
            print("üå± Ê§çÁâ©ÁóÖÊ∞ó„Å®Á¢∫Ë™ç„Åï„Çå„Åü„Åü„ÇÅ„ÄÅClassifierÁµêÊûú„ÇíÊé°Áî®")
            return {
                'prediction': top_predictions[0][0]['class_name'],
                'confidence': max_confidence,
                'method': 'classifier_confirmed',
                'reason': f'VisionÁîüÊàê„Å´„Çà„ÇäÊ§çÁâ©ÁóÖÊ∞ó„Å®Á¢∫Ë™ç (ÂàÜÈ°û‰ø°È†ºÂ∫¶: {max_confidence:.1%})',
                'top_alternatives': top_predictions[0][:3],
                'vision_confirmation': True,
                'vision_response': vision_response
            }
        else:
            # Ê§çÁâ©ÁóÖÊ∞óÈñ¢ÈÄ£„Åß„Å™„ÅÑ ‚Üí VisionÁîüÊàê„ÅßË©≥Á¥∞Ë™¨Êòé
            print("üìù ‰∏ÄËà¨ÁîªÂÉè„Å®„Åó„Å¶„ÄÅVisionÁîüÊàê„ÅßË©≥Á¥∞Ë™¨Êòé„Çí‰ΩúÊàê‰∏≠...")
            description = generate_detailed_description(image)
            return {
                'description': description,
                'method': 'vision_generation',
                'reason': 'VisionÁîüÊàê„Å´„Çà„ÇäÊ§çÁâ©ÁóÖÊ∞ó‰ª•Â§ñ„Å®Âà§ÂÆö',
                'confidence': 0.8,  # ÁîüÊàê„Éô„Éº„Çπ
                'vision_analysis': True,
                'vision_response': vision_response
            }

def compute_vision_metrics(eval_pred):
    """VisionÂàÜÈ°ûÁî®„ÅÆË©ï‰æ°„É°„Éà„É™„ÇØ„Çπ"""
    predictions, labels = eval_pred
    
    # predictions „ÅØ logits „ÅÆÂΩ¢ (batch_size, num_classes)
    pred_ids = np.argmax(predictions, axis=1)
    
    # Âü∫Êú¨ÁöÑ„Å™„É°„Éà„É™„ÇØ„Çπ
    accuracy = accuracy_score(labels, pred_ids)
    
    # Top-3 accuracy
    top3_preds = np.argsort(predictions, axis=1)[:, -3:]
    top3_accuracy = np.mean([label in top3_preds[i] for i, label in enumerate(labels)])
    
    # „ÇØ„É©„ÇπÂà•Á≤æÂ∫¶ (ÂêÑ„ÇØ„É©„Çπ„ÅÆrecall)
    cm = confusion_matrix(labels, pred_ids)
    class_accuracies = cm.diagonal() / cm.sum(axis=1)
    class_accuracies = np.nan_to_num(class_accuracies)  # 0„ÅßÂâ≤„Å£„ÅüÂ†¥Âêà„ÅÆNaN„Çí0„Å´Â§âÊèõ
    
    metrics = {
        'accuracy': accuracy,
        'top3_accuracy': top3_accuracy,
        'mean_class_accuracy': np.mean(class_accuracies),
        'min_class_accuracy': np.min(class_accuracies),
        'max_class_accuracy': np.max(class_accuracies),
        'median_class_accuracy': np.median(class_accuracies),
        'std_class_accuracy': np.std(class_accuracies),
    }
    
    return metrics

class VisionTrainer(Trainer):
    """VisionÂàÜÈ°ûÁî®„ÅÆËªΩÈáèTrainer - compute_metrics„ÇíÂ¶®„Åí„Å™„ÅÑ"""
    
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        pixel_values = inputs.get('pixel_values')
        labels = inputs.get('labels')
        
        # „É¢„Éá„É´„ÅÆforward()„Åßlogits„ÇíÂèñÂæó
        logits = model(pixel_values)
        
        # Cross entropy loss„ÇíË®àÁÆó
        loss = F.cross_entropy(logits, labels)
        
        if return_outputs:
            # Trainer„ÅåÊúüÂæÖ„Åô„ÇãÂΩ¢Âºè„Åßoutputs„ÇíËøî„Åô
            outputs = type('ModelOutput', (), {
                'loss': loss,
                'logits': logits,
            })()
            return loss, outputs
        return loss
    
    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):
        """Ë©ï‰æ°ÊôÇ„ÅÆ‰∫àÊ∏¨„Çπ„ÉÜ„ÉÉ„Éó„Çí„Ç™„Éº„Éê„Éº„É©„Ç§„Éâ"""
        pixel_values = inputs.get('pixel_values')
        labels = inputs.get('labels')
        
        with torch.no_grad():
            # „É¢„Éá„É´„ÅÆ‰∫àÊ∏¨„ÇíÂèñÂæó
            logits = model(pixel_values)
            loss = F.cross_entropy(logits, labels)
        
        # prediction_loss_only„ÅåTrue„ÅÆÂ†¥Âêà„ÅØloss„ÅÆ„ÅøËøî„Åô
        if prediction_loss_only:
            return (loss, None, None)
        
        # compute_metrics„Åß‰ΩøÁî®„Åô„Çã„Åü„ÇÅ„ÄÅlogits„Å®labels„ÇíËøî„Åô
        return (loss, logits, labels)

# VisionÂàÜÈ°ûÁî®„ÅÆ„Éá„Éº„Çø„Ç≥„É¨„Éº„Çø„Éº
class VisionDataCollator:
    def __init__(self, processor):
        self.processor = processor
    
    def __call__(self, batch):
        # ÁîªÂÉè„Å®„É©„Éô„É´„ÇíÂàÜÈõ¢
        images = [item["image"] for item in batch]
        labels = torch.tensor([item["labels"] for item in batch], dtype=torch.long)
        
        # ÁîªÂÉè„ÇíÂá¶ÁêÜ
        # Gemma3nProcessor„Çí‰ΩøÁî®„Åó„Å¶ÁîªÂÉè„Çípixel_values„Å´Â§âÊèõ
        pixel_values = []
        for image in images:
            # „ÉÄ„Éü„Éº„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„Å®ÁµÑ„ÅøÂêà„Çè„Åõ„Å¶Âá¶ÁêÜ
            processed = self.processor(
                text="dummy",  # „ÉÄ„Éü„Éº„ÉÜ„Ç≠„Çπ„Éà
                images=image,
                return_tensors="pt"
            )
            pixel_values.append(processed["pixel_values"].squeeze(0))
        
        pixel_values = torch.stack(pixel_values)
        
        return {
            "pixel_values": pixel_values,
            "labels": labels
        }

# Vision Tower„ÇíÂèñÂæó„Åó„Å¶VisionClassifier„Çí‰ΩúÊàê
print("üîç Extracting Vision Tower from model...")

# Áõ¥Êé•model„Åã„Çâvision_tower„ÇíÂèñÂæó
vision_tower = model.model.vision_tower

# Hidden dimension„ÇíÂèñÂæó
hidden_dim = vision_tower.config.hidden_size if hasattr(vision_tower, 'config') else 3584  # Gemma3n„ÅÆ„Éá„Éï„Ç©„É´„Éà

print(f"Vision Tower: {type(vision_tower)}")
print(f"Hidden dimension: {hidden_dim}")
print(f"Number of classes: {len(class_names)}")

# VisionCNNClassifier„Çí‰ΩúÊàê
vision_classifier = VisionCNNClassifier(
    vision_model=vision_tower,
    num_classes=len(class_names),
    hidden_dim=hidden_dim
)

# VisionÂàÜÈ°ûÁî®„ÅÆ„Éà„É¨„Éº„Éä„Éº„Çí‰ΩúÊàê
vision_trainer = VisionTrainer(
    model=vision_classifier,
    tokenizer=tokenizer,
    data_collator=VisionDataCollator(tokenizer),
    train_dataset=train_vision_dataset,
    eval_dataset=val_vision_dataset,
    compute_metrics=compute_vision_metrics,
    callbacks=[ClearMemoryCallback(clear_every_n_steps=50)],
    args=TrainingArguments(
        per_device_train_batch_size=8,  
        per_device_eval_batch_size=16,
        gradient_accumulation_steps=8,  
        warmup_steps=5,
        max_steps=100, # „Çà„ÇäÈï∑„ÅÑÂ≠¶Áøí„ÅßÊÄßËÉΩÂêë‰∏ä
        learning_rate=1e-3,  # VisionÂàÜÈ°û„Åß„ÅØÂ∞ë„ÅóÈ´ò„ÇÅ„ÅÆLR
        fp16=not is_bf16_supported(),
        bf16=is_bf16_supported(),
        logging_steps=5,
        logging_strategy="steps",
        logging_first_step=True,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="./vision_classification_outputs",
        
        report_to="wandb",
        run_name="gemma3n-vision-classification",
        
        eval_strategy="steps",
        eval_steps=25,
        eval_delay=20,
        
        save_strategy="steps",
        save_steps=50,
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model="eval_accuracy",
        greater_is_better=True,
        
        dataloader_pin_memory=False,
        dataloader_num_workers=0,
        remove_unused_columns=False,
    ),
)

print("üöÄ Vision Classification Trainer created!")
print(f"üìä Training samples: {len(train_vision_dataset)}")
print(f"üìä Validation samples: {len(val_vision_dataset)}")

# Start Vision Classification Training
vision_trainer.train()

print("üéâ Vision Classification Training completed!")
print(f"üíæ Model saved to: ./vision_classification_outputs")
print(f"üìù Model trained with Vision Classification for {len(class_names)} classes")

model_type = "vision_tower_classification"
log_data = {
    "training_status": "completed", 
    "model_type": model_type,
    "num_classes": len(class_names),
    "hidden_dim": hidden_dim
}

# Log training completion to WandB
wandb.log(log_data)

#%%
# Test the model
print("\nüß™ Testing model...")

# Test the vision classifier
test_sample = val_vision_dataset[0]
test_image = test_sample["image"]
test_label = test_sample["labels"]
actual_class = class_names[test_label]

print(f"Testing on: {actual_class} (label: {test_label})")

# Prepare input for vision classifier
with torch.no_grad():
    # Process image using data collator
    batch = vision_trainer.data_collator([test_sample])
    pixel_values = batch["pixel_values"].to("cuda")
    
    # Get prediction
    logits = vision_classifier(pixel_values=pixel_values)
    predicted_class_id = torch.argmax(logits, dim=-1).item()
    predicted_class = class_names[predicted_class_id]
    
    # Get confidence scores
    probabilities = torch.softmax(logits, dim=-1)
    confidence = probabilities[0, predicted_class_id].item()
    
    print(f"Predicted: {predicted_class} (confidence: {confidence:.3f})")

print(f"‚úÖ Vision classification test completed!")
print("=" * 50)

# Test ID/Label conversion functions
print("\nüîß Testing ID/Label conversion functions...")
print("-" * 50)

# Define conversion functions here for Jupyter compatibility
def id_to_label_local(class_id, class_names_list=None):
    """„ÇØ„É©„ÇπID„Çí„ÇØ„É©„ÇπÂêç„Å´Â§âÊèõ"""
    if class_names_list is None:
        class_names_list = class_names
    return class_names_list[class_id]

def ids_to_labels_local(class_ids, class_names_list=None):
    """„ÇØ„É©„ÇπID„É™„Çπ„Éà/ÈÖçÂàó„Çí„ÇØ„É©„ÇπÂêç„É™„Çπ„Éà„Å´Â§âÊèõ"""
    if class_names_list is None:
        class_names_list = class_names
    return [class_names_list[id] for id in class_ids]

def get_top_k_predictions_local(logits, k=3, class_names_list=None):
    """logits„Åã„ÇâTop-K‰∫àÊ∏¨„Å®„Åù„ÅÆÁ¢∫‰ø°Â∫¶„ÇíËøî„Åô"""
    import torch
    
    if class_names_list is None:
        class_names_list = class_names
    
    # logits„Åå1Ê¨°ÂÖÉ„ÅÆÂ†¥Âêà(Âçò‰∏Ä„Çµ„É≥„Éó„É´)„ÅØ2Ê¨°ÂÖÉ„Å´Â§âÊèõ
    if len(logits.shape) == 1:
        logits = logits.unsqueeze(0)
    
    # Á¢∫Áéá„Å´Â§âÊèõ„Åó„Å¶Top-KÂèñÂæó
    probs = torch.softmax(logits, dim=-1)
    top_k_probs, top_k_ids = torch.topk(probs, k, dim=-1)
    
    results = []
    for i in range(len(top_k_ids)):
        predictions = []
        for j in range(k):
            class_id = top_k_ids[i][j].item()
            confidence = top_k_probs[i][j].item()
            class_name = class_names_list[class_id]
            predictions.append({
                'class_id': class_id,
                'class_name': class_name,
                'confidence': confidence
            })
        results.append(predictions)
    
    return results

# Test 1: id_to_label function
test_id = predicted_class_id
converted_label = id_to_label_local(test_id)
print(f"üìç Test 1 - id_to_label({test_id}) = '{converted_label}'")

# Test 2: ids_to_labels function
test_ids = [0, 5, 10, predicted_class_id]
converted_labels = ids_to_labels_local(test_ids)
print(f"üìç Test 2 - ids_to_labels({test_ids}):")
for i, label in enumerate(converted_labels):
    print(f"   ID {test_ids[i]} ‚Üí '{label}'")

# Test 3: get_top_k_predictions function
top3_predictions = get_top_k_predictions_local(logits, k=3)
print(f"üìç Test 3 - Top-3 predictions:")
for i, pred in enumerate(top3_predictions[0]):  # ÊúÄÂàù„ÅÆ„Çµ„É≥„Éó„É´„ÅÆTop-3
    print(f"   {i+1}. {pred['class_name']} (ID: {pred['class_id']}, confidence: {pred['confidence']:.3f})")

print("\n" + "=" * 70)
print("üß† „Çπ„Éû„Éº„ÉàÈÅ©ÂøúÁöÑ‰∫àÊ∏¨„Ç∑„Çπ„ÉÜ„É†„ÅÆ„ÉÜ„Çπ„Éà")
print("=" * 70)

# Test smart prediction system
print(f"\nüéØ „ÉÜ„Çπ„ÉàÁîªÂÉè„Åß„Çπ„Éû„Éº„Éà‰∫àÊ∏¨„ÇíÂÆüË°å...")
print(f"üñºÔ∏è „ÉÜ„Çπ„ÉàÂØæË±°: {actual_class}")

# Execute smart prediction
smart_result = smart_predict_image(test_image)

# Display results
print(f"\nüìä „Çπ„Éû„Éº„Éà‰∫àÊ∏¨ÁµêÊûú:")
print(f"   ‰ΩøÁî®ÊâãÊ≥ï: {smart_result['method']}")
print(f"   ÁêÜÁî±: {smart_result['reason']}")

if 'prediction' in smart_result:
    print(f"   ‰∫àÊ∏¨„ÇØ„É©„Çπ: {smart_result['prediction']}")
    print(f"   ‰ø°È†ºÂ∫¶: {smart_result['confidence']:.1%}")
    if 'top_alternatives' in smart_result:
        print("   ‰ª£ÊõøÂÄôË£ú:")
        for i, alt in enumerate(smart_result['top_alternatives'][:3], 1):
            print(f"      {i}. {alt['class_name']} ({alt['confidence']:.1%})")

if 'description' in smart_result:
    print(f"   ÁîüÊàê„Åï„Çå„ÅüË™¨Êòé: {smart_result['description']}")

if 'vision_response' in smart_result:
    print(f"   VisionÂà§ÂÆöË©≥Á¥∞: {smart_result['vision_response']}")

# Ê≠£Ëß£„Å®ÊØîËºÉ
if 'prediction' in smart_result:
    is_correct = smart_result['prediction'] == actual_class
    print(f"\nüéØ ‰∫àÊ∏¨ÁµêÊûú: {'‚úÖ Ê≠£Ëß£' if is_correct else '‚ùå ‰∏çÊ≠£Ëß£'}")
    print(f"   ÂÆüÈöõ: {actual_class}")
    print(f"   ‰∫àÊ∏¨: {smart_result['prediction']}")

print("\n" + "=" * 70)

# Display the test image
display(test_image)

print(f"üè∑Ô∏è Actual class: {actual_class}")
print(f"üîç Predicted class: {predicted_class}")
print(f"üéØ Confidence: {confidence:.1%}")

# Utility functions for detailed logging
def log_prediction_result(step_name: str, actual_label: str, predicted_label: str, 
                         response_text: str, confidence: float, is_correct: bool,
                         image=None, step_number: int = None):
    """Log detailed prediction result to WandB"""
    log_data = {
        f"{step_name}/actual_label": actual_label,
        f"{step_name}/predicted_label": predicted_label or "None",
        f"{step_name}/response_text": response_text,
        f"{step_name}/confidence": confidence,
        f"{step_name}/is_correct": is_correct,
    }
    
    if step_number is not None:
        log_data[f"{step_name}/step_number"] = step_number
    
    if image is not None:
        log_data[f"{step_name}/test_image"] = wandb.Image(image, caption=f"Actual: {actual_label}")
    
    wandb.log(log_data)

# Import statement moved above with other imports

def update_class_statistics(class_stats: dict, actual_class: str, predicted_class: str, is_correct: bool):
    """Update running class-wise statistics"""
    if actual_class not in class_stats:
        class_stats[actual_class] = {"total": 0, "correct": 0, "predictions": []}
    
    class_stats[actual_class]["total"] += 1
    if is_correct:
        class_stats[actual_class]["correct"] += 1
    class_stats[actual_class]["predictions"].append(predicted_class or "None")
    
    return class_stats

# Enhanced prediction extraction for adaptive approach
def extract_predicted_label(response_text: str, available_classes: list) -> tuple:
    """Extract predicted class from response using text-based matching
    
    Args:
        response_text: Model response text
        available_classes: List of valid class names
        
    Returns:
        tuple: (predicted_class, confidence_score)
    """
    from difflib import SequenceMatcher
    
    response_clean = response_text.strip()
    
    # Exact match (highest confidence)
    for class_name in available_classes:
        if class_name == response_clean:
            return class_name, 0.9
    
    # Case-insensitive exact match
    response_lower = response_clean.lower()
    for class_name in available_classes:
        if class_name.lower() == response_lower:
            return class_name, 0.85
    
    # Substring match
    for class_name in available_classes:
        if class_name in response_clean:
            return class_name, 0.8
        if class_name.lower() in response_lower:
            return class_name, 0.75
    
    # Fuzzy matching for robustness
    best_match = None
    best_score = 0.0
    
    for class_name in available_classes:
        similarity = SequenceMatcher(None, response_lower, class_name.lower()).ratio()
        if similarity > best_score and similarity > 0.6:
            best_score = similarity
            best_match = class_name
    
    if best_match:
        confidence = best_score * 0.7
        return best_match, confidence
    
    # No valid prediction found
    print(f"‚ö†Ô∏è No valid text-based prediction found in response: '{response_clean}'")
    return available_classes[0] if available_classes else None, 0.1

# Already have predicted_class and confidence from vision classifier

# Find examples of the predicted class from original dataset
combined_dataset = train_dataset + val_dataset
predicted_class_samples = [item for item in combined_dataset if item['class'] == predicted_class]

if predicted_class_samples:
    print(f"\nüìö Reference images for '{predicted_class}' (showing 3 examples):")
    print("-" * 50)
    
    # Show up to 3 reference images
    reference_samples = random.sample(predicted_class_samples, min(3, len(predicted_class_samples)))
    
    for i, ref_sample in enumerate(reference_samples, 1):
        ref_image = Image.open(ref_sample["path"])
        if ref_image.mode != 'RGB':
            ref_image = ref_image.convert('RGB')
        
        print(f"Reference {i}: {os.path.basename(ref_sample['path'])}")
        display(ref_image)
        print()
else:
    print(f"‚ùå No reference images found for '{predicted_class}'")

# Check if prediction is correct
is_correct = (predicted_class == actual_class)
is_partial_match = False  # Not applicable for direct classification

result_emoji = "‚úÖ" if is_correct else ("üü°" if is_partial_match else "‚ùå")
accuracy_status = "Exact Match" if is_correct else ("Partial Match" if is_partial_match else "Incorrect")
print(f"{result_emoji} Prediction: {accuracy_status}")

# Log detailed single test result
log_prediction_result(
    step_name="single_test",
    actual_label=actual_class,
    predicted_label=predicted_class,
    response_text=f"Vision classification: {predicted_class}",
    confidence=confidence,
    is_correct=is_correct,
    image=test_image,
    step_number=1
)

# Log to WandB
wandb_log_data = {
    "single_test/test_image": wandb.Image(test_image, caption=f"Test: {actual_class}"),
    "single_test/actual_class": actual_class,
    "single_test/predicted_class": predicted_class,
    "single_test/confidence": confidence,
    "single_test/is_correct": is_correct
}

# Log reference images if available
if predicted_class_samples:
    for j, ref_sample in enumerate(reference_samples[:2], 1):  # Log first 2 reference images
        ref_img = Image.open(ref_sample["path"])
        if ref_img.mode != 'RGB':
            ref_img = ref_img.convert('RGB')
        wandb_log_data[f"single_test/reference_{j}"] = wandb.Image(
            ref_img, 
            caption=f"Ref {j}: {predicted_class}"
        )

wandb.log(wandb_log_data)
wandb.finish()

print(f"\nüéâ Vision classification test completed successfully!")
print(f"Model achieved {confidence:.1%} confidence on the prediction.")

# %%
# Vision classification implementation completed
# - Replaced text generation approach with direct vision classification  
# - Used Vision Tower + Linear classifier
# - Implemented proper data collation and training
# - Added comprehensive testing with confidence scores
# Show all class names for caption generation reference