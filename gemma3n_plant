#%%
# Vision Fine-tuning for plnat Classification - CLAUDE.md Based

!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo
!pip install --upgrade bitsandbytes
!pip install triton==3.2.0
!pip install pip3-autoremove
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
!pip install unsloth
!pip install faiss-cpu
!pip install sentence-transformers
!pip install wikipedia
!pip install --no-deps git+https://github.com/huggingface/transformers.git
!pip install --no-deps --upgrade timm
!pip uninstall -y deepspeed
!pip install deepspeed==0.14.4
!pip install wandb
!pip install fastai

#%%
import os
import random
import pandas as pd
import json
from typing import List
from torchvision import datasets
from unsloth import FastModel, is_bf16_supported, FastVisionModel
from unsloth.trainer import UnslothVisionDataCollator
from trl import SFTTrainer, SFTConfig
import wandb
from PIL import Image
from IPython.display import display, Image as IPImage

os.environ["CUDA_VISIBLE_DEVICES"] = "0, 1"
os.environ["WANDB_API_KEY"] = "abd0ce2837eca439d66f6c603136603c1729cd3e"

#%%
# Initialize WandB with environment variable login
wandb.login(key=os.getenv("WANDB_API_KEY"))

wandb.init(
    project="gemma3n-plant-classification",
    name="vision-finetuning-experiment",
    config={
        "model_name": "unsloth/gemma-3n-E2B-it",
        "max_seq_length": 1024,
        "load_in_4bit": True,
        "max_files_per_class": 50,
        "lora_rank": 16,
        "lora_alpha": 32,
    }
)

# Load the model (exactly as in CLAUDE.md)
model, tokenizer = FastModel.from_pretrained(
    model_name = "unsloth/gemma-3n-E2B-it",
    dtype = None,
    max_seq_length = 1024,
    load_in_4bit = True,
    full_finetuning = False,
)

#%%
# Data loading using existing train/valid split
base_dir = '/notebooks/kaggle/input/new_plant_diseases/2/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)'
train_dir = os.path.join(base_dir, 'train')
valid_dir = os.path.join(base_dir, 'valid')

max_files_per_class = 10000  # Smaller for testing

def load_dataset_from_dir(root_dir, max_files_per_class=None):
    """Load dataset from directory with optional file limit per class"""
    classes = []
    paths = []
    class_file_counts = {}
    
    for dirname, _, filenames in os.walk(root_dir):
        class_name = dirname.split('/')[-1]
        if class_name == os.path.basename(root_dir):  # Skip root directory
            continue
            
        if class_name not in class_file_counts:
            class_file_counts[class_name] = 0
        
        for filename in filenames:
            if max_files_per_class is None or class_file_counts[class_name] < max_files_per_class:
                if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):
                    full_path = os.path.join(dirname, filename)
                    if os.path.exists(full_path):
                        classes.append(class_name)
                        paths.append(full_path)
                        class_file_counts[class_name] += 1
    
    return classes, paths, class_file_counts

# Load train and validation datasets separately
train_classes, train_paths, train_counts = load_dataset_from_dir(train_dir, max_files_per_class)
val_classes, val_paths, val_counts = load_dataset_from_dir(valid_dir, max_files_per_class)

# Get unified class names from ImageFolder for consistency
train_dataset_folder = datasets.ImageFolder(root=train_dir)
class_names = train_dataset_folder.classes
print(f"Class names: {class_names}")
print(f"Number of classes: {len(class_names)}")

# Create mapping
N = list(range(len(class_names)))
normal_mapping = dict(zip(class_names, N))
reverse_mapping = dict(zip(N, class_names))

# Create train dataset
train_dataset = []
for path, class_name in zip(train_paths, train_classes):
    train_dataset.append({"path": path, "class": class_name})

# Create validation dataset
val_dataset = []
for path, class_name in zip(val_paths, val_classes):
    val_dataset.append({"path": path, "class": class_name})

print(f"Training samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")
print(f"Train class distribution: {train_counts}")
print(f"Val class distribution: {val_counts}")

#%%
# Convert to conversation format (exactly as in CLAUDE.md)
instruction = "You are an expert of plant disease. Describe accurately what you see in this image."

def convert_to_conversation(sample):
    try:
        # Load image as PIL Image object instead of path string
        image = Image.open(sample["path"])
        # Convert to RGB if needed (handles RGBA, grayscale, etc.)
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        conversation = [
            { "role": "user",
              "content" : [
                {"type" : "text",  "text"  : instruction},
                {"type" : "image", "image" : image} ]
            },
            { "role" : "assistant",
              "content" : [
                {"type" : "text",  "text"  : sample["class"]} ]
            },
        ]
        return { "messages" : conversation }
    except Exception as e:
        print(f"‚ùå Error loading image {sample['path']}: {e}")
        return None

#%%
# Convert train and validation datasets to conversation format separately
train_converted_dataset = [convert_to_conversation(sample) for sample in train_dataset]
train_converted_dataset = [item for item in train_converted_dataset if item is not None]
print(f"‚úÖ Converted {len(train_converted_dataset)} training samples to conversation format")

val_converted_dataset = [convert_to_conversation(sample) for sample in val_dataset]
val_converted_dataset = [item for item in val_converted_dataset if item is not None]
print(f"‚úÖ Converted {len(val_converted_dataset)} validation samples to conversation format")

# Show sample
print("\nüìã Sample training conversation:")
print(train_converted_dataset[0])

print(f"\nüìä Final dataset split:")
print(f"   Training samples: {len(train_converted_dataset)}")
print(f"   Validation samples: {len(val_converted_dataset)}")

#%%
# Vision Fine-tuning (exactly as in CLAUDE.md)
print("\nüöÄ Starting Vision Fine-tuning...")

# Add PEFT configuration for quantized model
model = FastVisionModel.get_peft_model(
    model,
    finetune_vision_layers = True,
    finetune_language_layers = False,
    finetune_attention_modules = True,
    finetune_mlp_modules = True,
    r = 32,  # LoRA rank
    lora_alpha = 64,
    lora_dropout = 0,  # Changed from 0.1 to 0 for Unsloth compatibility
    bias = "none",
    # target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj","vision_embed","tok_embeddings"],
    random_state = 3407,
    use_gradient_checkpointing = "unsloth",
)

# Enable for training
FastVisionModel.for_training(model)

# Create trainer with validation dataset and evaluation settings
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    data_collator = UnslothVisionDataCollator(model, tokenizer), # Must use!
    train_dataset = train_converted_dataset,  # Use converted training data
    eval_dataset = val_converted_dataset,     # Use converted validation data
    args = SFTConfig(
        per_device_train_batch_size = 8,  # Further reduced for stability
        per_device_eval_batch_size = 2,   # Validation batch size
        gradient_accumulation_steps = 8,  # Increased to maintain effective batch size
        warmup_steps = 5,
        max_steps = 100,  # Very short for testing
        learning_rate = 1e-4,
        fp16 = not is_bf16_supported(),
        bf16 = is_bf16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "cosine",
        seed = 3407,
        output_dir = "./plant_vision_outputs",
        report_to = "wandb",
        run_name = "gemma3n-plant-vision-ft",
        use_cpu = False,   # Don't use CPU, stay on GPU
        dataloader_pin_memory = False,  # Disable pin memory to avoid conflicts
        dataloader_num_workers = 0,    # Disable multiprocessing to avoid issues
        
        # Evaluation settings
        eval_strategy = "steps",         # Enable evaluation
        eval_steps = 25,                 # Evaluate every 25 steps
        save_strategy = "steps",         # Save every save_steps
        save_steps = 50,                 # Less frequent saves
        load_best_model_at_end = True,   # Load best model at end
        metric_for_best_model = "eval_loss",  # Use eval loss for best model

        # You MUST put the below items for vision finetuning (from CLAUDE.md):
        remove_unused_columns = False,
        dataset_text_field = "",
        dataset_kwargs = {"skip_prepare_dataset": True},
        dataset_num_proc = 1,
        max_seq_length = 2048,
    ),
)

# Start training
print("üìà Training started...")
trainer.train()

print("üéâ Training completed!")
print(f"üíæ Model saved to: ./plnat_vision_outputs")

# Log training completion to WandB
wandb.log({"training_status": "completed"})
wandb.finish()

#%%
# Test the model
print("\nüß™ Testing model...")

# Get a test sample and load as PIL Image (use validation dataset)
test_sample = val_dataset[0]
test_image = Image.open(test_sample["path"])
if test_image.mode != 'RGB':
    test_image = test_image.convert('RGB')

test_messages = [{
    "role": "user",
    "content": [
        {"type": "text", "text": "What type of plnat is this?"},
        {"type": "image", "image": test_image}
    ]
}]

# Generate response
inputs = tokenizer.apply_chat_template(
    test_messages,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt"
).to("cuda")

outputs = model.generate(
    **inputs,
    max_new_tokens=50,
    temperature=0.7,
    do_sample=True,
    pad_token_id=tokenizer.eos_token_id
)

response_tokens = outputs[0][inputs['input_ids'].shape[1]:]
response = tokenizer.decode(response_tokens, skip_special_tokens=True)

print(f"‚úÖ Test completed!")
print(f"\nüñºÔ∏è Test Image: {os.path.basename(test_sample['path'])}")
print("=" * 50)

# Display the test image
display(test_image)

print(f"üè∑Ô∏è Actual class: {test_sample['class']}")
print(f"ü§ñ Model response: {response}")

# Extract predicted class name from response
def extract_predicted_class(response_text, available_classes):
    """Extract the most likely class name from model response"""
    response_lower = response_text.lower()
    
    # Look for exact matches first
    for class_name in available_classes:
        if class_name.lower() in response_lower:
            return class_name
    
    # If no exact match, look for partial matches
    for class_name in available_classes:
        class_words = class_name.lower().split()
        if any(word in response_lower for word in class_words if len(word) > 3):
            return class_name
    
    return None

class_names = train_dataset_folder.classes
predicted_class = extract_predicted_class(response, class_names)

if predicted_class:
    print(f"üîç Extracted predicted class: {predicted_class}")
    
    # Find examples of the predicted class (from combined dataset)
    combined_dataset = train_dataset + val_dataset
    predicted_class_samples = [item for item in combined_dataset if item['class'] == predicted_class]
    
    if predicted_class_samples:
        print(f"\nüìö Reference images for '{predicted_class}' (showing 3 examples):")
        print("-" * 50)
        
        # Show up to 3 reference images
        reference_samples = random.sample(predicted_class_samples, min(3, len(predicted_class_samples)))
        
        for i, ref_sample in enumerate(reference_samples, 1):
            ref_image = Image.open(ref_sample["path"])
            if ref_image.mode != 'RGB':
                ref_image = ref_image.convert('RGB')
            
            print(f"Reference {i}: {os.path.basename(ref_sample['path'])}")
            display(ref_image)
            print()
    else:
        print(f"‚ùå No reference images found for '{predicted_class}'")
else:
    print("‚ùì Could not extract a clear class prediction from the response")

# Check if prediction is correct
is_correct = test_sample['class'].lower() in response.lower()
result_emoji = "‚úÖ" if is_correct else "‚ùå"
print(f"{result_emoji} Prediction: {'Correct' if is_correct else 'Incorrect'}")

# Log to WandB
wandb_log_data = {
    "single_test/test_image": wandb.Image(test_image, caption=f"Test: {test_sample['class']}"),
    "single_test/actual_class": test_sample['class'],
    "single_test/predicted_response": response,
    "single_test/is_correct": is_correct
}

if predicted_class:
    wandb_log_data["single_test/extracted_class"] = predicted_class
    
    # Log reference images if available
    if predicted_class_samples:
        for j, ref_sample in enumerate(reference_samples[:2], 1):  # Log first 2 reference images
            ref_img = Image.open(ref_sample["path"])
            if ref_img.mode != 'RGB':
                ref_img = ref_img.convert('RGB')
            wandb_log_data[f"single_test/reference_{j}"] = wandb.Image(
                ref_img, 
                caption=f"Ref {j}: {predicted_class}"
            )

wandb.log(wandb_log_data)

#%%
# Multiple test cases from different classes
print("\nüß™ Testing multiple samples from different classes...")
print("=" * 60)

# Get unique classes and select one sample from each (from combined dataset)
combined_dataset = train_dataset + val_dataset
unique_classes = list(set([item['class'] for item in combined_dataset]))
test_samples = []

for class_name in unique_classes[:5]:  # Test first 5 classes
    class_samples = [item for item in combined_dataset if item['class'] == class_name]
    if class_samples:
        test_samples.append(random.choice(class_samples))

# Create WandB Table for multiple test results
table_columns = ["Test Image", "Actual Class", "Predicted Response", "Extracted Class", "Is Correct", "Reference 1", "Reference 2"]
test_table = wandb.Table(columns=table_columns)
correct_count = 0

for i, sample in enumerate(test_samples, 1):
    print(f"\nüì∏ Test Case {i}/5: {sample['class']}")
    print("-" * 40)
    
    # Load and display image
    test_img = Image.open(sample["path"])
    if test_img.mode != 'RGB':
        test_img = test_img.convert('RGB')
    
    display(test_img)
    
    # Generate prediction
    messages = [{
        "role": "user",
        "content": [
            {"type": "text", "text": "What type of plnat is this?"},
            {"type": "image", "image": test_img}
        ]
    }]
    
    inputs = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt"
    ).to("cuda")
    
    outputs = model.generate(
        **inputs,
        max_new_tokens=50,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    
    response_tokens = outputs[0][inputs['input_ids'].shape[1]:]
    prediction = tokenizer.decode(response_tokens, skip_special_tokens=True)
    
    # Display results
    print(f"üè∑Ô∏è Actual: {sample['class']}")
    print(f"ü§ñ Predicted: {prediction}")
    
    # Extract and show reference images for predicted class
    predicted_class = extract_predicted_class(prediction, class_names)
    
    if predicted_class:
        print(f"üîç Extracted class: {predicted_class}")
        predicted_class_samples = [item for item in combined_dataset if item['class'] == predicted_class]
        
        if predicted_class_samples and len(predicted_class_samples) > 0:
            print(f"üìö Reference examples for '{predicted_class}':")
            reference_samples = random.sample(predicted_class_samples, min(2, len(predicted_class_samples)))
            
            for j, ref_sample in enumerate(reference_samples, 1):
                ref_image = Image.open(ref_sample["path"])
                if ref_image.mode != 'RGB':
                    ref_image = ref_image.convert('RGB')
                
                print(f"  Ref {j}: {os.path.basename(ref_sample['path'])}")
                display(ref_image)
    
    is_match = sample['class'].lower() in prediction.lower()
    status_emoji = "‚úÖ" if is_match else "‚ùå"
    print(f"{status_emoji} {'CORRECT' if is_match else 'INCORRECT'}")
    
    if is_match:
        correct_count += 1
    
    # Add row to WandB table
    table_row = [
        wandb.Image(test_img, caption=f"Test: {sample['class']}"),
        sample['class'],
        prediction,
        predicted_class if predicted_class else "N/A",
        is_match
    ]
    
    # Add reference images to table
    if predicted_class and predicted_class_samples:
        for k in range(2):  # Add up to 2 reference images
            if k < len(reference_samples):
                ref_img = Image.open(reference_samples[k]["path"])
                if ref_img.mode != 'RGB':
                    ref_img = ref_img.convert('RGB')
                table_row.append(wandb.Image(ref_img, caption=f"Ref: {predicted_class}"))
            else:
                table_row.append(None)  # Empty cell if no reference image
    else:
        table_row.extend([None, None])  # Empty cells for reference images
    
    test_table.add_data(*table_row)
    
    if i < len(test_samples):
        print("\n" + "="*60)

# Summary statistics
accuracy = (correct_count / len(test_samples)) * 100 if test_samples else 0

print(f"\nüìä Test Summary:")
print("=" * 30)
print(f"üß¨ Classes tested: {len(test_samples)}")
print(f"üî¨ Total available classes: {len(unique_classes)}")
print(f"üéØ Accuracy: {accuracy:.1f}% ({correct_count}/{len(test_samples)})")

# Log comprehensive results to WandB
wandb.log({
    "multiple_tests/test_results_table": test_table,
    "multiple_tests/accuracy": accuracy,
    "multiple_tests/correct_predictions": correct_count,
    "multiple_tests/total_tests": len(test_samples),
    "multiple_tests/total_classes": len(unique_classes)
})
# %%
import torch
import gc

# del model
# del trainer
torch.cuda.empty_cache()
torch.cuda.ipc_collect()
gc.collect()

# %%
