#%%
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

!pip install pip3-autoremove
!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu124
!pip install unsloth
!pip install faiss-cpu
!pip install sentence-transformers
!pip install wikipedia
!pip install --no-deps git+https://github.com/huggingface/transformers.git
!pip install --no-deps --upgrade timm
#%%
import torch
import pandas as pd
import random
import numpy as np
import faiss
import wikipedia
import json
import re
import time
from dataclasses import dataclass
from typing import Optional, List, Dict, Any, Tuple
from unsloth import FastModel
from transformers import TextStreamer
from torchvision import datasets, transforms, models
from sentence_transformers import SentenceTransformer

# Load the model
model, tokenizer = FastModel.from_pretrained(
    model_name = "unsloth/gemma-3n-E2B-it",
    dtype = None,
    max_seq_length = 1024,
    load_in_4bit = True,
    full_finetuning = False,
)

# Helper function for inference
def do_gemma_3n_inference(messages, max_new_tokens = 128):
    inputs = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt = True,
        tokenize = True,
        return_dict = True,
        return_tensors = "pt",
    ).to("cuda")
    
    # å¸¸ã«å‡ºåŠ›ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ã¦è¿”ã™
    outputs = model.generate(
        **inputs,
        max_new_tokens = max_new_tokens,
        temperature = 1.0, top_p = 0.95, top_k = 64,
        do_sample = True,
        pad_token_id = tokenizer.eos_token_id,
    )
    # æ–°ã—ãç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã‚’æŠ½å‡º
    response_tokens = outputs[0][inputs['input_ids'].shape[1]:]
    response_text = tokenizer.decode(response_tokens, skip_special_tokens=True)
    return response_text

#%%
%%time
dir0='/kaggle/input/mushroom1/merged_dataset'
max_files_per_class = 1000  # Limit files per class to control total dataset size

classes=[]
paths=[]
class_file_counts = {}
for dirname, _, filenames in os.walk(dir0):
    class_name = dirname.split('/')[-1]
    if class_name not in class_file_counts:
        class_file_counts[class_name] = 0
    
    for filename in filenames:
        if class_file_counts[class_name] < max_files_per_class:
            classes+=[class_name]
            paths+=[(os.path.join(dirname, filename))]
            class_file_counts[class_name] += 1

dataset0=datasets.ImageFolder(root=dir0)
class_names=dataset0.classes
print(class_names)
print(len(class_names))

N=list(range(len(classes)))
normal_mapping=dict(zip(class_names,N))
reverse_mapping=dict(zip(N,class_names))

data=pd.DataFrame(columns=['path','class','label'])
data['path']=paths
data['class']=classes
data['label']=data['class'].map(normal_mapping)
print(len(data))

def create_path_label_list(df):
    path_label_list = []
    for _, row in df.iterrows():
        path = row['path']
        label = row['label']
        path_label_list.append((path, label))
    return path_label_list

path_label = create_path_label_list(data)
path_label = random.sample(path_label,20000)
print(len(path_label))
print(path_label[0:3])

image_path, label = path_label[0]

#%%
# Vision Fine-tuning Implementation for Mushroom Classification
from unsloth import FastVisionModel
from unsloth.trainer import UnslothVisionDataCollator
from transformers import TrainingArguments
from trl import SFTTrainer
from datasets import Dataset
from PIL import Image
import torch.nn.functional as F

class MushroomVisionDataset:
    """ã‚­ãƒã‚³ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, image_paths: List[str], labels: List[str], class_names: List[str]):
        """
        Args:
            image_paths: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            labels: ã‚¯ãƒ©ã‚¹åã®ãƒªã‚¹ãƒˆ
            class_names: å…¨ã‚¯ãƒ©ã‚¹åã®ãƒªã‚¹ãƒˆ
        """
        self.image_paths = image_paths
        self.labels = labels
        self.class_names = class_names
        self.label_to_id = {name: i for i, name in enumerate(class_names)}
    
    def prepare_vision_dataset(self, train_ratio: float = 0.8, max_samples_per_class: int = 10) -> tuple:
        """
        Vision fine-tuningç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
        
        Args:
            train_ratio: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ¯”ç‡
            max_samples_per_class: ã‚¯ãƒ©ã‚¹ã‚ãŸã‚Šã®æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°
            
        Returns:
            (train_dataset, val_dataset)
        """
        print(f"ğŸ“Š Vision fine-tuningç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™é–‹å§‹...")
        
        # ã‚¯ãƒ©ã‚¹åˆ¥ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†
        class_data = {}
        for path, label in zip(self.image_paths, self.labels):
            if label not in class_data:
                class_data[label] = []
            class_data[label].append(path)
        
        # å„ã‚¯ãƒ©ã‚¹ã‹ã‚‰å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        selected_data = []
        for class_name, paths in class_data.items():
            if len(paths) > 0:
                # ã‚¯ãƒ©ã‚¹ã‚ãŸã‚Šã®æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°ã«åˆ¶é™
                sampled_paths = random.sample(paths, min(len(paths), max_samples_per_class))
                for path in sampled_paths:
                    selected_data.append({
                        'image_path': path,
                        'class_name': class_name,
                        'class_id': self.label_to_id.get(class_name, -1),
                        'text': f"This is a {class_name} mushroom."
                    })
        
        print(f"   é¸æŠã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿æ•°: {len(selected_data)}")
        print(f"   å¯¾è±¡ã‚¯ãƒ©ã‚¹æ•°: {len(class_data)}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        random.shuffle(selected_data)
        
        # è¨“ç·´/æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
        split_idx = int(len(selected_data) * train_ratio)
        train_data = selected_data[:split_idx]
        val_data = selected_data[split_idx:]
        
        print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_data)}")
        print(f"   æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_data)}")
        
        # Hugging Face Datasetå½¢å¼ã«å¤‰æ›
        train_dataset = Dataset.from_list(train_data)
        val_dataset = Dataset.from_list(val_data)
        
        return train_dataset, val_dataset

def formatting_func_for_vision(examples):
    """Vision fine-tuningç”¨ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆé–¢æ•°"""
    texts = []
    for text in examples['text']:
        texts.append(text)
    return {"text": texts}

class MushroomVisionFineTuner:
    """ã‚­ãƒã‚³ç”»åƒåˆ†é¡ç”¨Vision Fine-tunerã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, base_model, base_tokenizer):
        """
        Args:
            base_model: ãƒ™ãƒ¼ã‚¹ã®Gemma3nãƒ¢ãƒ‡ãƒ«
            base_tokenizer: ãƒ™ãƒ¼ã‚¹ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        """
        self.base_model = base_model
        self.base_tokenizer = base_tokenizer
        self.vision_model = None
        self.is_fine_tuned = False
    
    def setup_vision_model(self, finetune_vision_layers: bool = True, 
                          finetune_language_layers: bool = False):
        """
        Vision fine-tuningç”¨ãƒ¢ãƒ‡ãƒ«è¨­å®š
        
        Args:
            finetune_vision_layers: Visionå±¤ã‚’fine-tuningã™ã‚‹ã‹
            finetune_language_layers: Languageå±¤ã‚’fine-tuningã™ã‚‹ã‹
        """
        print(f"ğŸ”§ Vision fine-tuningç”¨ãƒ¢ãƒ‡ãƒ«è¨­å®šä¸­...")
        
        try:
            # FastVisionModelã§PEFTè¨­å®š
            self.vision_model = FastVisionModel.get_peft_model(
                self.base_model,
                finetune_vision_layers=finetune_vision_layers,
                finetune_language_layers=finetune_language_layers,
                finetune_attention_modules=True,
                finetune_mlp_modules=True,
                r=16,  # LoRA rank
                lora_alpha=32,
                lora_dropout=0.1,
                target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
                use_gradient_checkpointing="unsloth",
                random_state=42,
            )
            print(f"âœ… Vision modelè¨­å®šå®Œäº†")
            print(f"   Visionå±¤ fine-tuning: {finetune_vision_layers}")
            print(f"   Languageå±¤ fine-tuning: {finetune_language_layers}")
            
        except Exception as e:
            print(f"âŒ Vision modelè¨­å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise e
    
    def train_vision_model(self, train_dataset, val_dataset, 
                          output_dir: str = "./mushroom_vision_model",
                          num_epochs: int = 3,
                          batch_size: int = 2,
                          learning_rate: float = 2e-5):
        """
        Vision modelã‚’è¨“ç·´
        
        Args:
            train_dataset: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            val_dataset: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            output_dir: ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            num_epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            learning_rate: å­¦ç¿’ç‡
        """
        if self.vision_model is None:
            raise ValueError("Vision modelãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚setup_vision_model()ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        
        print(f"ğŸš€ Vision modelè¨“ç·´é–‹å§‹...")
        print(f"   ã‚¨ãƒãƒƒã‚¯æ•°: {num_epochs}")
        print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
        print(f"   å­¦ç¿’ç‡: {learning_rate}")
        
        try:
            # è¨“ç·´å¼•æ•°è¨­å®š
            training_args = TrainingArguments(
                output_dir=output_dir,
                num_train_epochs=num_epochs,
                per_device_train_batch_size=batch_size,
                per_device_eval_batch_size=batch_size,
                learning_rate=learning_rate,
                warmup_steps=50,
                logging_steps=10,
                evaluation_strategy="steps",
                eval_steps=50,
                save_strategy="steps",
                save_steps=100,
                load_best_model_at_end=True,
                metric_for_best_model="eval_loss",
                greater_is_better=False,
                remove_unused_columns=False,
                dataloader_pin_memory=False,
                gradient_checkpointing=True,
                fp16=True,
                report_to="none",  # WandBãªã©ã®ãƒ­ã‚°ã‚’ç„¡åŠ¹
            )
            
            # SFTTrainerè¨­å®š
            trainer = SFTTrainer(
                model=self.vision_model,
                tokenizer=self.base_tokenizer,
                train_dataset=train_dataset,
                eval_dataset=val_dataset,
                formatting_func=formatting_func_for_vision,
                data_collator=UnslothVisionDataCollator(tokenizer=self.base_tokenizer),
                args=training_args,
                max_seq_length=512,
            )
            
            # è¨“ç·´å®Ÿè¡Œ
            print(f"ğŸ“ˆ è¨“ç·´é–‹å§‹...")
            trainer.train()
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            print(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­: {output_dir}")
            trainer.save_model(output_dir)
            self.base_tokenizer.save_pretrained(output_dir)
            
            self.is_fine_tuned = True
            print(f"âœ… Vision modelè¨“ç·´å®Œäº†ï¼")
            
            # è¨“ç·´çµæœã‚µãƒãƒªãƒ¼
            train_result = trainer.state.log_history
            if train_result:
                final_loss = train_result[-1].get('eval_loss', 'N/A')
                print(f"   æœ€çµ‚è©•ä¾¡æå¤±: {final_loss}")
            
        except Exception as e:
            print(f"âŒ è¨“ç·´ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise e
    
    def load_fine_tuned_model(self, model_path: str):
        """
        Fine-tunedãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        
        Args:
            model_path: ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        """
        try:
            print(f"ğŸ“‚ Fine-tunedãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­: {model_path}")
            
            # FastVisionModelã§ãƒ­ãƒ¼ãƒ‰
            self.vision_model, self.base_tokenizer = FastVisionModel.from_pretrained(
                model_name=model_path,
                dtype=None,
                max_seq_length=1024,
                load_in_4bit=True,
            )
            
            self.is_fine_tuned = True
            print(f"âœ… Fine-tunedãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise e
    
    def get_model_for_inference(self):
        """æ¨è«–ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—"""
        if self.is_fine_tuned and self.vision_model is not None:
            return self.vision_model
        else:
            return self.base_model

#%%
# RAG (Retrieval-Augmented Generation) Implementation
class WikipediaRAG:
    def __init__(self, embedding_model='all-MiniLM-L6-v2'):
        """
        Initialize RAG system with Wikipedia as knowledge base
        """
        self.embedding_model = SentenceTransformer(embedding_model)
        self.documents = []
        self.embeddings = None
        self.index = None

    def search_wikipedia(self, query: str, num_results: int = 5) -> List[str]:
        """
        Search Wikipedia and return page contents
        """
        try:
            search_results = wikipedia.search(query, results=num_results)
            documents = []
            
            for title in search_results:
                try:
                    page = wikipedia.page(title)
                    # Split content into chunks for better retrieval
                    content = page.content
                    chunks = self._split_text(content, chunk_size=500)
                    for chunk in chunks:
                        if len(chunk.strip()) > 100:  # Filter out very short chunks
                            documents.append({
                                'title': title,
                                'content': chunk,
                                'url': page.url
                            })
                except wikipedia.exceptions.DisambiguationError as e:
                    # Take the first option if disambiguation occurs
                    try:
                        page = wikipedia.page(e.options[0])
                        content = page.content
                        chunks = self._split_text(content, chunk_size=500)
                        for chunk in chunks:
                            if len(chunk.strip()) > 100:
                                documents.append({
                                    'title': e.options[0],
                                    'content': chunk,
                                    'url': page.url
                                })
                    except:
                        continue
                except:
                    continue
                    
            return documents
        except:
            return []
    
    def _split_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """
        Split text into overlapping chunks
        """
        # Clean text
        text = re.sub(r'\s+', ' ', text.strip())
        
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            # Try to break at sentence boundaries
            if end < len(text):
                # Look for sentence endings
                sentence_end = text.rfind('.', start, end)
                if sentence_end > start + chunk_size // 2:
                    end = sentence_end + 1
            
            chunks.append(text[start:end].strip())
            start = end - overlap
            
        return chunks

    def search_species_dynamically(self, species_candidates: List, knowledge_base_type: str = "general") -> List[Dict]:
        """æ¨å®šç¨®ã«å¯¾ã™ã‚‹å‹•çš„Wikipediaæ¤œç´¢"""
        
        dynamic_documents = []
        
        for i, candidate in enumerate(species_candidates[:3]):  # ä¸Šä½3å€™è£œã®ã¿
            species_name = candidate.name
            confidence = candidate.score
            
            # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹åˆ¥ã®ç‰¹åŒ–ã‚¯ã‚¨ãƒªç”Ÿæˆ
            species_queries = self._generate_domain_specific_queries(species_name, knowledge_base_type)
            
            for query in species_queries:
                try:
                    search_results = self.search_wikipedia(query, num_results=2)
                    
                    for doc in search_results:
                        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ç¨®æƒ…å ±ã‚’è¿½åŠ 
                        doc['species_name'] = species_name
                        doc['species_confidence'] = confidence
                        doc['knowledge_domain'] = knowledge_base_type
                        doc['query_type'] = self._extract_query_type(query, knowledge_base_type)
                        dynamic_documents.append(doc)
                        
                except Exception as e:
                    continue
        
        return dynamic_documents

    def _generate_domain_specific_queries(self, species_name: str, knowledge_base_type: str) -> List[str]:
        """ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ"""
        clean_name = species_name.strip()
        
        if knowledge_base_type == "field_guides":
            # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚¬ã‚¤ãƒ‰ï¼šè­˜åˆ¥ãƒ»å½¢æ…‹å­¦é‡è¦–
            queries = [
                f"{clean_name} field guide identification",
                f"{clean_name} mushroom identification key features",
                f"{clean_name} morphology cap stem gills pores",
                f"{clean_name} distinguishing characteristics",
                f"{clean_name} lookalike similar species"
            ]
        
        elif knowledge_base_type == "toxicity_reports":
            # æ¯’æ€§ãƒ¬ãƒãƒ¼ãƒˆï¼šå®‰å…¨æ€§ãƒ»æ¯’æ€§é‡è¦–
            queries = [
                f"{clean_name} toxicity poisonous edible",
                f"{clean_name} mushroom safety consumption",
                f"{clean_name} toxic compounds effects",
                f"{clean_name} poisoning symptoms treatment",
                f"{clean_name} edibility safety warnings"
            ]
        
        elif knowledge_base_type == "cooking_recipes":
            # èª¿ç†ãƒ¬ã‚·ãƒ”ï¼šé£Ÿç”¨ãƒ»æ–™ç†é‡è¦–
            queries = [
                f"{clean_name} cooking recipes preparation",
                f"{clean_name} edible mushroom culinary uses",
                f"{clean_name} cooking methods recipes",
                f"{clean_name} food preparation techniques",
                f"{clean_name} mushroom recipes cuisine"
            ]
        
        else:
            # æ±ç”¨æ¤œç´¢ï¼ˆåŸºæœ¬çš„ãªç¨®æƒ…å ±ï¼‰
            clean_name = species_name.strip()
            if any(word in clean_name.lower() for word in ['species', 'mushroom', 'unknown']):
                # ä¸å®Œå…¨ãªç¨®åã®å ´åˆã¯å±åã®ã¿ã‚’ä½¿ç”¨
                genus = clean_name.split()[0] if ' ' in clean_name else clean_name
                queries = [
                    f"{genus} mushroom identification",
                    f"{genus} genus characteristics", 
                    f"{genus} mushroom safety edibility"
                ]
            else:
                # å®Œå…¨ãªç¨®åã®å ´åˆ
                queries = [
                    f"{clean_name} mushroom identification characteristics",
                    f"{clean_name} morphology features",
                    f"{clean_name} toxicity safety edibility",
                    f"{clean_name} habitat distribution ecology"
                ]
        
        return queries

    def _extract_query_type(self, query: str, knowledge_base_type: str) -> str:
        """
        ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—ã‚’æŠ½å‡ºï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰
        """
        if "identification" in query or "morphology" in query:
            return "identification"
        elif "toxicity" in query or "poisonous" in query or "safety" in query:
            return "toxicity"
        elif "cooking" in query or "recipe" in query or "culinary" in query:
            return "cooking"
        elif "habitat" in query or "distribution" in query:
            return "ecology"
        else:
            return knowledge_base_type

    def retrieve(self, query: str, top_k: int = 3) -> List[Dict]:
        """
        Retrieve relevant documents for a query
        """
        if self.index is None:
            return []
        
        # Encode query
        query_embedding = self.embedding_model.encode([query])
        faiss.normalize_L2(query_embedding)
        
        # Search
        scores, indices = self.index.search(query_embedding, top_k)
        
        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx < len(self.documents):
                doc = self.documents[idx].copy()
                doc['relevance_score'] = float(score)
                results.append(doc)
        
        return results

    def generate_context(self, query: str, top_k: int = 3) -> str:
        """
        Generate context string from retrieved documents
        """
        retrieved_docs = self.retrieve(query, top_k)
        
        if not retrieved_docs:
            return "No relevant information found."
        
        context_parts = []
        for i, doc in enumerate(retrieved_docs):
            context_parts.append(f"Source {i+1} ({doc['title']}):\n{doc['content']}")
        
        return "\n\n".join(context_parts)

    def rag_generate(self, query: str, top_k: int = 3) -> Dict:
        """
        Generate response using RAG approach
        """
        # Retrieve relevant context
        context = self.generate_context(query, top_k)
        
        # Create enhanced prompt with context
        enhanced_prompt = f"""Based on the following context information, please answer the question.

Context:
{context}

Question: {query}

Answer:"""
        
        return {
            'context': context,
            'enhanced_prompt': enhanced_prompt,
            'retrieved_docs': self.retrieve(query, top_k)
        }

# %%
# Mobile Mushroom Identification Workflow
@dataclass
class WorkflowResult:
    """3ã‚¹ãƒ†ãƒƒãƒ—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµæœ"""
    candidate_species: List[str]
    similarity_scores: List[float]
    toxicity_info: Dict[str, Any]
    cooking_methods: Dict[str, Any]
    safety_warnings: List[str]
    final_answer: str
    recommendation: str

class ImageDatabaseRAG:
    """ç”»åƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®é¡ä¼¼ç”»åƒæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  (Gemma3nä½¿ç”¨)"""
    
    def __init__(self, image_dataset_path: str, gemma_model, gemma_tokenizer):
        """
        Args:
            image_dataset_path: ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹
            gemma_model: Gemma3nãƒ¢ãƒ‡ãƒ«
            gemma_tokenizer: Gemma3nãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        """
        self.dataset_path = image_dataset_path
        self.gemma_model = gemma_model
        self.gemma_tokenizer = gemma_tokenizer
        self.image_embeddings = None
        self.image_metadata = []
        self.index = None

    def build_image_index(self, class_names: List[str], paths: List[str], 
                         classes: List[str]):
        """
        ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ï¼ˆå„ã‚¯ãƒ©ã‚¹5æšã¾ã§å‡ç­‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
        
        Args:
            class_names: ã‚¯ãƒ©ã‚¹åãƒªã‚¹ãƒˆ
            paths: ç”»åƒãƒ‘ã‚¹ãƒªã‚¹ãƒˆ  
            classes: å„ç”»åƒã®ã‚¯ãƒ©ã‚¹
        """
        # å„ã‚¯ãƒ©ã‚¹ã‹ã‚‰å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå„ã‚¯ãƒ©ã‚¹5æšã¾ã§ï¼‰
        images_per_class = 5
        selected_indices = []
        
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†æ:")
        print(f"   ç·ç”»åƒæ•°: {len(classes)}æš")
        print(f"   ç·ã‚¯ãƒ©ã‚¹æ•°: {len(class_names)}ç¨®é¡")
        
        # ã‚¯ãƒ©ã‚¹åˆ¥ã®ç”»åƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åé›†ã—å‡ç­‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        class_indices = {}
        for i, class_name in enumerate(classes):
            if class_name not in class_indices:
                class_indices[class_name] = []
            class_indices[class_name].append(i)
        
        print(f"   å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‚¯ãƒ©ã‚¹æ•°: {len(class_indices)}ç¨®é¡")
        
        # ã‚¯ãƒ©ã‚¹åˆ¥ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æƒ…å ±ã‚’è¡¨ç¤º
        sampled_classes = 0
        for class_name in class_indices.keys():
            available = len(class_indices[class_name])
            to_select = min(available, images_per_class)
            if to_select > 0:
                selected_for_class = random.sample(class_indices[class_name], to_select)
                selected_indices.extend(selected_for_class)
                sampled_classes += 1
                if sampled_classes <= 10:  # æœ€åˆã®10ã‚¯ãƒ©ã‚¹ã ã‘è©³ç´°è¡¨ç¤º
                    print(f"   ğŸ“ {class_name}: {available}æšä¸­{to_select}æšé¸æŠ")
        
        print(f"   ğŸ¯ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°çµæœ: {sampled_classes}ã‚¯ãƒ©ã‚¹ã‹ã‚‰{len(selected_indices)}æšé¸æŠ")
        
        embeddings = []
        metadata = []
        total_images = len(selected_indices)
        
        print(f"ğŸš€ ç”»åƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰é–‹å§‹: {total_images}æšã®ç”»åƒã‚’å‡¦ç†ä¸­...")
        
        # é€²æ—ãƒãƒ¼ç”¨å¤‰æ•°
        successful_extractions = 0
        failed_extractions = 0
        
        for i, idx in enumerate(selected_indices):
            try:
                image_path = paths[idx]
                class_name = classes[idx]
                
                # é€²æ—ãƒãƒ¼è¡¨ç¤º
                progress = (i + 1) / total_images * 100
                bar_length = 30
                filled_length = int(bar_length * (i + 1) // total_images)
                bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
                print(f"\rğŸ“¸ [{i+1:3d}/{total_images}] [{bar}] {progress:5.1f}% | {class_name[:20]:<20}", end='', flush=True)
                
                # Gemma3nã§ç”»åƒã‹ã‚‰ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º
                image_embedding = self._extract_image_features(image_path, class_name, verbose=False)
                
                if image_embedding is not None:
                    embeddings.append(image_embedding)
                    metadata.append({
                        'path': image_path,
                        'class': class_name,
                        'class_id': class_names.index(class_name) if class_name in class_names else -1,
                        'index': idx
                    })
                    successful_extractions += 1
                else:
                    failed_extractions += 1
                
            except Exception as e:
                failed_extractions += 1
                continue
        
        # é€²æ—ãƒãƒ¼å®Œäº†
        print(f"\nâœ… ç”»åƒå‡¦ç†å®Œäº†: æˆåŠŸ {successful_extractions}ä»¶, å¤±æ•— {failed_extractions}ä»¶")
        
        if embeddings:
            print(f"ğŸ”§ FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ä¸­...")
            self.image_embeddings = np.array(embeddings)
            self.image_metadata = metadata

            # 2æ¬¡å…ƒé…åˆ—ã«å¤‰æ›ï¼ˆFAISSè¦ä»¶ï¼‰
            if self.image_embeddings.ndim != 2:
                # 1æ¬¡å…ƒé…åˆ—ã®å ´åˆã¯ reshapeã—ã¦2æ¬¡å…ƒã«ã™ã‚‹
                if self.image_embeddings.ndim == 1:
                    self.image_embeddings = self.image_embeddings.reshape(1, -1)
                else:
                    # 3æ¬¡å…ƒä»¥ä¸Šã®å ´åˆã¯ flatten
                    self.image_embeddings = self.image_embeddings.reshape(len(embeddings), -1)
            
            # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
            dimension = self.image_embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)
            self.image_embeddings = np.ascontiguousarray(self.image_embeddings, dtype=np.float32)
            faiss.normalize_L2(self.image_embeddings)
            self.index.add(self.image_embeddings)
            
            print(f"ğŸ‰ FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å®Œäº†!")
            print(f"   ğŸ“Š ç™»éŒ²ç”»åƒæ•°: {len(self.image_metadata)}æš")
            print(f"   ğŸ“ ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {dimension}æ¬¡å…ƒ")
            
            # ç™»éŒ²ã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã®è©³ç´°åˆ†æ
            registered_classes = {}
            for m in self.image_metadata:
                class_name = m['class']
                if class_name not in registered_classes:
                    registered_classes[class_name] = 0
                registered_classes[class_name] += 1
            
            print(f"   ğŸ·ï¸ ç™»éŒ²ã‚¯ãƒ©ã‚¹æ•°: {len(registered_classes)}ç¨®é¡")
            print(f"   ğŸ“‹ ç™»éŒ²ã‚¯ãƒ©ã‚¹è©³ç´°:")
            for i, (class_name, count) in enumerate(registered_classes.items()):
                if i < 20:  # æœ€åˆã®20ã‚¯ãƒ©ã‚¹ã ã‘è¡¨ç¤º
                    print(f"      {i+1:2d}. {class_name}: {count}æš")
                elif i == 20:
                    print(f"      ... ä»–{len(registered_classes)-20}ã‚¯ãƒ©ã‚¹")
        else:
            print(f"âŒ ç”»åƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å¤±æ•—: æœ‰åŠ¹ãªç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ãŒ0å€‹")

    def _extract_image_features(self, image_path: str, class_name: str, verbose: bool = True) -> Optional[np.ndarray]:
        """
        Gemma3nã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‹ã‚‰ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º
        
        Args:
            image_path: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            class_name: ç”»åƒã®ã‚¯ãƒ©ã‚¹å
            verbose: è©³ç´°ãƒ­ã‚°ã®è¡¨ç¤º
            
        Returns:
            ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ« (numpyé…åˆ—)
        """
        try:
            if verbose:
                print(f"ğŸ” Gemma3nã§ç”»åƒç‰¹å¾´æŠ½å‡ºä¸­: {image_path} (ã‚¯ãƒ©ã‚¹: {class_name})")
            
            # Gemma3nã«ç”»åƒã®ç‰¹å¾´æŠ½å‡ºã‚’ä¾é ¼
            feature_messages = [{
                "role": "user",
                "content": [
                    {"type": "image", "image": image_path},
                    {"type": "text", "text": "Extract visual features of this mushroom. Describe in exactly 50 words: cap color, shape, size, stem characteristics, gills/pores, texture, and any distinctive markings."}
                ]
            }]
            
            # Gemma3nã§ç‰¹å¾´ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¦å®Ÿéš›ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º
            inputs = self.gemma_tokenizer.apply_chat_template(
                feature_messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt"
            ).to("cuda")
            
            # ãƒ¢ãƒ‡ãƒ«ã®ä¸­é–“å±¤ã‹ã‚‰ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º
            with torch.no_grad():
                outputs = self.gemma_model(**inputs, output_hidden_states=True)
                # æœ€å¾Œã®éš ã‚Œå±¤ã®å¹³å‡ã‚’ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦ä½¿ç”¨
                hidden_states = outputs.hidden_states[-1]  # [batch, seq_len, hidden_dim]
                feature_vector = hidden_states.mean(dim=1).squeeze().cpu().numpy()  # [hidden_dim]
            
            if verbose:
                print(f"âœ… ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«æŠ½å‡ºå®Œäº†: {feature_vector.shape}")
            return feature_vector
            
        except Exception as e:
            if verbose:
                print(f"âš ï¸ Gemma3nç‰¹å¾´æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)}")
                print(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ“¬ä¼¼ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ç”¨ (ã‚¯ãƒ©ã‚¹: {class_name})")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¯ãƒ©ã‚¹åãƒ™ãƒ¼ã‚¹ã®æ“¬ä¼¼ãƒ™ã‚¯ãƒˆãƒ«
            return self._create_fallback_vector(class_name)

    def _create_fallback_vector(self, class_name: str, dim: int = 768) -> np.ndarray:
        """
        ã‚¯ãƒ©ã‚¹åã‹ã‚‰æ“¬ä¼¼ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
        
        Args:
            class_name: ã‚¯ãƒ©ã‚¹å
            dim: ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°
            
        Returns:
            æ“¬ä¼¼ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«
        """
        # ã‚¯ãƒ©ã‚¹åã®ãƒãƒƒã‚·ãƒ¥ã‚’ã‚·ãƒ¼ãƒ‰ã«ã—ã¦å®‰å®šã—ãŸæ“¬ä¼¼ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
        hash_seed = hash(class_name) % (2**31)
        np.random.seed(hash_seed)
        vector = np.random.normal(0, 1, dim)
        np.random.seed()  # ã‚·ãƒ¼ãƒ‰ã‚’ãƒªã‚»ãƒƒãƒˆ
        return vector.astype(np.float32)

    def find_similar_images(self, query_image_path: str, top_k: int = 5) -> List[Dict]:
        """
        ã‚¯ã‚¨ãƒªç”»åƒã«é¡ä¼¼ã—ãŸç”»åƒã‚’æ¤œç´¢
        
        Args:
            query_image_path: ã‚¯ã‚¨ãƒªç”»åƒã®ãƒ‘ã‚¹
            top_k: å–å¾—ã™ã‚‹é¡ä¼¼ç”»åƒæ•°
            
        Returns:
            é¡ä¼¼ç”»åƒã®æƒ…å ±ãƒªã‚¹ãƒˆ
        """
        if self.index is None:
            return []

        try:
            # Gemma3nã§ã‚¯ã‚¨ãƒªç”»åƒã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º
            query_vector = self._extract_image_features(query_image_path, "query", verbose=False)
            
            if query_vector is None:
                return []
            
            # ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ­£è¦åŒ–ã—ã¦FAISSç”¨ã«æº–å‚™
            query_embedding = query_vector.reshape(1, -1).astype(np.float32)
            faiss.normalize_L2(query_embedding)
            
            # é¡ä¼¼ç”»åƒæ¤œç´¢
            scores, indices = self.index.search(query_embedding, top_k)
            
            similar_images = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx < len(self.image_metadata):
                    metadata = self.image_metadata[idx].copy()
                    metadata['similarity_score'] = float(score)
                    similar_images.append(metadata)
            
            return similar_images
            
        except Exception as e:
            return []

    def get_class_examples(self, class_name: str, max_examples: int = 3) -> List[Dict]:
        """
        æŒ‡å®šã‚¯ãƒ©ã‚¹ã®ä»£è¡¨ç”»åƒã‚’å–å¾—
        
        Args:
            class_name: ã‚¯ãƒ©ã‚¹å
            max_examples: å–å¾—ã™ã‚‹ä¾‹æ•°
            
        Returns:
            ã‚¯ãƒ©ã‚¹ä¾‹ç”»åƒã®ãƒªã‚¹ãƒˆ
        """
        examples = []
        for metadata in self.image_metadata:
            if metadata['class'] == class_name and len(examples) < max_examples:
                examples.append(metadata)
        
        return examples

    def display_image_comparison(self, query_image_path: str, similar_images: List[Dict], top_n: int = 3):
        """
        ãƒ†ã‚¹ãƒˆç”¨: ã‚¯ã‚¨ãƒªç”»åƒã¨é¡ä¼¼ç”»åƒã‚’æ¯”è¼ƒè¡¨ç¤º
        
        Args:
            query_image_path: ãƒ†ã‚¹ãƒˆç”»åƒã®ãƒ‘ã‚¹
            similar_images: é¡ä¼¼æ¤œç´¢çµæœ
            top_n: è¡¨ç¤ºã™ã‚‹é¡ä¼¼ç”»åƒæ•°
        """
        try:
            from IPython.display import Image, display
            
            print(f"\nğŸ“Š ç”»åƒæ¯”è¼ƒçµæœ:")
            print(f"ğŸ¯ ãƒ†ã‚¹ãƒˆç”»åƒ: {query_image_path}")
            print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å: {query_image_path.split('/')[-1]}")
            
            # ãƒ†ã‚¹ãƒˆç”»åƒã‚’è¡¨ç¤º
            try:
                display(Image(filename=query_image_path, width=300))
            except Exception as e:
                print(f"âŒ ãƒ†ã‚¹ãƒˆç”»åƒè¡¨ç¤ºä¸å¯: {str(e)}")
            
            print(f"\nğŸ” é¡ä¼¼ç”»åƒ Top {min(top_n, len(similar_images))}:")
            for i, img in enumerate(similar_images[:top_n]):
                similarity_percent = img['similarity_score'] * 100
                print(f"\n  {i+1}. {img['class']} (é¡ä¼¼åº¦: {similarity_percent:.1f}%)")
                print(f"     ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {img['path'].split('/')[-1]}")
                print(f"     ğŸ—‚ï¸  ãƒ•ãƒ«ãƒ‘ã‚¹: {img['path']}")
                
                # é¡ä¼¼åº¦ã®è¦–è¦šçš„è¡¨ç¤º
                bar_length = 20
                filled_length = int(bar_length * img['similarity_score'])
                bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
                print(f"     ğŸ“Š [{bar}] {similarity_percent:.1f}%")
                
                # é¡ä¼¼ç”»åƒã‚’è¡¨ç¤º
                try:
                    display(Image(filename=img['path'], width=300))
                except Exception as e:
                    print(f"     âŒ ç”»åƒè¡¨ç¤ºä¸å¯: {str(e)}")
        
        except ImportError:
            # IPython.displayãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆã®ã¿è¡¨ç¤º
            print(f"\nğŸ“Š ç”»åƒæ¯”è¼ƒçµæœ (ãƒ†ã‚­ã‚¹ãƒˆã®ã¿):")
            print(f"ğŸ¯ ãƒ†ã‚¹ãƒˆç”»åƒ: {query_image_path}")
            print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å: {query_image_path.split('/')[-1]}")
            
            print(f"\nğŸ” é¡ä¼¼ç”»åƒ Top {min(top_n, len(similar_images))}:")
            for i, img in enumerate(similar_images[:top_n]):
                similarity_percent = img['similarity_score'] * 100
                print(f"  {i+1}. {img['class']} (é¡ä¼¼åº¦: {similarity_percent:.1f}%)")
                print(f"     ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {img['path'].split('/')[-1]}")
                print(f"     ğŸ—‚ï¸  ãƒ•ãƒ«ãƒ‘ã‚¹: {img['path']}")
                
                # é¡ä¼¼åº¦ã®è¦–è¦šçš„è¡¨ç¤º
                bar_length = 20
                filled_length = int(bar_length * img['similarity_score'])
                bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
                print(f"     ğŸ“Š [{bar}] {similarity_percent:.1f}%")
                print()


class MobileMushroomWorkflow:
    """ãƒ¢ãƒã‚¤ãƒ«ç«¯æœ«å‘ã‘ã‚­ãƒã‚³è­˜åˆ¥ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµ±åˆã‚¯ãƒ©ã‚¹ - 3ã‚¹ãƒ†ãƒƒãƒ—ç‰ˆ + Vision Fine-tuningå¯¾å¿œ"""
    
    def __init__(self, model, tokenizer, embedding_model):
        # 3ã‚¹ãƒ†ãƒƒãƒ—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç”¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        
        # Base models
        self.base_model = model
        self.base_tokenizer = tokenizer
        
        # Vision Fine-tuning
        self.vision_finetuner = MushroomVisionFineTuner(model, tokenizer)
        
        # Step 1: ç”»åƒãƒãƒƒãƒãƒ³ã‚°ï¼ˆbase modelã§ã‚¹ã‚¿ãƒ¼ãƒˆã€å¾Œã§fine-tunedã«å¤‰æ›´å¯èƒ½ï¼‰
        self.image_db = ImageDatabaseRAG("dataset", model, tokenizer)
        
        # Step 2: Wikipediaæ¤œç´¢
        self.wikipedia_rag = WikipediaRAG('all-MiniLM-L6-v2')
        
        # Fine-tuningçŠ¶æ…‹ç®¡ç†
        self.use_fine_tuned_model = False

    def run_vision_fine_tuning(self, image_paths: List[str], labels: List[str], 
                              class_names: List[str], output_dir: str = "./mushroom_vision_model",
                              max_samples_per_class: int = 10, num_epochs: int = 3):
        """
        Vision Fine-tuningã‚’å®Ÿè¡Œ
        
        Args:
            image_paths: ç”»åƒãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            labels: ãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆ
            class_names: ã‚¯ãƒ©ã‚¹åã®ãƒªã‚¹ãƒˆ
            output_dir: ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            max_samples_per_class: ã‚¯ãƒ©ã‚¹ã‚ãŸã‚Šã®æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°
            num_epochs: ã‚¨ãƒãƒƒã‚¯æ•°
        """
        print(f"ğŸ¯ Vision Fine-tuningé–‹å§‹...")
        print(f"   å¯¾è±¡ç”»åƒæ•°: {len(image_paths)}")
        print(f"   ã‚¯ãƒ©ã‚¹æ•°: {len(class_names)}")
        print(f"   ã‚¯ãƒ©ã‚¹ã‚ãŸã‚Šæœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°: {max_samples_per_class}")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
            dataset_manager = MushroomVisionDataset(image_paths, labels, class_names)
            train_dataset, val_dataset = dataset_manager.prepare_vision_dataset(
                max_samples_per_class=max_samples_per_class
            )
            
            # Vision modelã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            self.vision_finetuner.setup_vision_model(
                finetune_vision_layers=True,
                finetune_language_layers=False
            )
            
            # è¨“ç·´å®Ÿè¡Œ
            self.vision_finetuner.train_vision_model(
                train_dataset=train_dataset,
                val_dataset=val_dataset,
                output_dir=output_dir,
                num_epochs=num_epochs,
                batch_size=2,  # VRAMåˆ¶ç´„ã®ãŸã‚å°ã•ãè¨­å®š
                learning_rate=2e-5
            )
            
            # Fine-tunedãƒ¢ãƒ‡ãƒ«ã‚’æœ‰åŠ¹åŒ–
            self.use_fine_tuned_model = True
            self._update_models_to_fine_tuned()
            
            print(f"ğŸ‰ Vision Fine-tuningå®Œäº†ï¼")
            print(f"   ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {output_dir}")
            print(f"   Fine-tunedãƒ¢ãƒ‡ãƒ«ãŒæœ‰åŠ¹åŒ–ã•ã‚Œã¾ã—ãŸ")
            
        except Exception as e:
            print(f"âŒ Fine-tuningå¤±æ•—: {str(e)}")
            raise e
    
    def load_fine_tuned_model(self, model_path: str):
        """
        ä¿å­˜æ¸ˆã¿ã®Fine-tunedãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        
        Args:
            model_path: ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        """
        try:
            print(f"ğŸ“‚ Fine-tunedãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰: {model_path}")
            
            # Fine-tunedãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
            self.vision_finetuner.load_fine_tuned_model(model_path)
            
            # Fine-tunedãƒ¢ãƒ‡ãƒ«ã‚’æœ‰åŠ¹åŒ–
            self.use_fine_tuned_model = True
            self._update_models_to_fine_tuned()
            
            print(f"âœ… Fine-tunedãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {str(e)}")
            raise e
    
    def _update_models_to_fine_tuned(self):
        """å†…éƒ¨çš„ã«fine-tunedãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«åˆ‡ã‚Šæ›¿ãˆ"""
        if self.vision_finetuner.is_fine_tuned:
            fine_tuned_model = self.vision_finetuner.get_model_for_inference()
            # ImageDatabaseRAGã®ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°
            self.image_db.gemma_model = fine_tuned_model
            print(f"ğŸ”„ ImageDatabaseRAGã‚’Fine-tunedãƒ¢ãƒ‡ãƒ«ã«åˆ‡ã‚Šæ›¿ãˆ")
    
    def switch_to_base_model(self):
        """ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«æˆ»ã™"""
        self.use_fine_tuned_model = False
        self.image_db.gemma_model = self.base_model
        print(f"ğŸ”„ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«åˆ‡ã‚Šæ›¿ãˆ")
    
    def get_model_status(self):
        """ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ã‚’ç¢ºèª"""
        status = {
            'use_fine_tuned': self.use_fine_tuned_model,
            'fine_tuned_available': self.vision_finetuner.is_fine_tuned,
            'current_model': 'Fine-tuned' if self.use_fine_tuned_model else 'Base'
        }
        print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹:")
        for key, value in status.items():
            print(f"   {key}: {value}")
        return status

    def initialize_image_database(self, image_dataset_path: str, class_names: List[str], 
                                 paths: List[str], classes: List[str]):
        """
        ç”»åƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–ï¼ˆç°¡ç•¥åŒ–ç‰ˆï¼‰
        
        Args:
            image_dataset_path: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹
            class_names: ã‚¯ãƒ©ã‚¹åãƒªã‚¹ãƒˆ
            paths: ç”»åƒãƒ‘ã‚¹ãƒªã‚¹ãƒˆ
            classes: å„ç”»åƒã®ã‚¯ãƒ©ã‚¹
        """
        # 3ã‚¹ãƒ†ãƒƒãƒ—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç”¨ã«ç”»åƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰
        self.image_db.build_image_index(class_names, paths, classes)
        print(f"ğŸ“‚ ç”»åƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±:")
        print(f"  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹: {image_dataset_path}")
        print(f"  ã‚¯ãƒ©ã‚¹æ•°: {len(class_names)}")
        print(f"  ç”»åƒæ•°: {len(paths)}")

    def process_image(self, image_path: str, user_question: Optional[str] = None, 
                     verbose: bool = False) -> WorkflowResult:
        """
        3ã‚¹ãƒ†ãƒƒãƒ—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ
        
        Args:
            image_path: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            user_question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•
            verbose: é€²æ—è¡¨ç¤ºã®æœ‰åŠ¹/ç„¡åŠ¹
            
        Returns:
            WorkflowResult ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        start_time = time.time()
        
        try:
            if verbose:
                print("ğŸš€ 3ã‚¹ãƒ†ãƒƒãƒ—ã‚­ãƒã‚³è­˜åˆ¥ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–‹å§‹...")
            
            # Step 1: ç”»åƒãƒãƒƒãƒãƒ³ã‚°
            if verbose:
                print("ğŸ” Step 1: ç”»åƒãƒãƒƒãƒãƒ³ã‚°ä¸­...")
            similar_images = self.image_db.find_similar_images(image_path, top_k=5)
            
            # ãƒ‡ãƒãƒƒã‚°ç”¨: ç”»åƒæ¯”è¼ƒè¡¨ç¤º
            if verbose and similar_images:
                self.image_db.display_image_comparison(image_path, similar_images, top_n=3)
            
            # ä¸Šä½é¡ä¼¼ç”»åƒã‹ã‚‰å€™è£œç¨®ã‚’æŠ½å‡º
            candidate_species = [img.get('class', 'ä¸æ˜') for img in similar_images[:3]]
            similarity_scores = [img.get('similarity_score', 0.0) for img in similar_images[:3]]
            
            # Step 2: Wikipediaæ¤œç´¢
            if verbose:
                print("ğŸ“š Step 2: Wikipediaæ¤œç´¢ä¸­...")
            
            # å€™è£œç¨®ã«åŸºã¥ã„ã¦Wikipediaæ¤œç´¢
            toxicity_info = {}
            cooking_methods = {}
            safety_warnings = []
            
            for species in candidate_species:
                if species != 'ä¸æ˜':
                    # æ¯’æ€§æƒ…å ±æ¤œç´¢
                    toxicity_query = f"{species} toxicity poisonous edible safety"
                    toxicity_docs = self.wikipedia_rag.search_wikipedia(toxicity_query, num_results=2)
                    toxicity_info[species] = toxicity_docs
                    
                    # èª¿ç†æƒ…å ±æ¤œç´¢
                    cooking_query = f"{species} cooking preparation recipe"
                    cooking_docs = self.wikipedia_rag.search_wikipedia(cooking_query, num_results=2)
                    cooking_methods[species] = cooking_docs
                    
                    # å®‰å…¨è­¦å‘ŠæŠ½å‡º
                    for doc in toxicity_docs:
                        content = doc.get('content', '').lower()
                        if 'toxic' in content or 'poisonous' in content:
                            safety_warnings.append(f"{species}: æ¯’æ€§ã®å¯èƒ½æ€§ã‚ã‚Š")
            
            # Step 3: å›ç­”ç”Ÿæˆ
            if verbose:
                print("ğŸ“ Step 3: å›ç­”ç”Ÿæˆä¸­...")
            
            # æœ€çµ‚å›ç­”ã¨æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ
            final_answer = self._generate_answer(candidate_species, toxicity_info, cooking_methods, user_question)
            recommendation = self._generate_recommendation(candidate_species, safety_warnings)
            
            # çµæœä½œæˆ
            total_time = time.time() - start_time
            
            result = WorkflowResult(
                candidate_species=candidate_species,
                similarity_scores=similarity_scores,
                toxicity_info=toxicity_info,
                cooking_methods=cooking_methods,
                safety_warnings=safety_warnings,
                final_answer=final_answer,
                recommendation=recommendation
            )
            
            if verbose:
                print(f"ğŸ‰ 3ã‚¹ãƒ†ãƒƒãƒ—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Œäº†! (ç·å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’)")

            return result
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
            total_time = time.time() - start_time
            if verbose:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            
            return WorkflowResult(
                candidate_species=["ã‚¨ãƒ©ãƒ¼"],
                similarity_scores=[0.0],
                toxicity_info={},
                cooking_methods={},
                safety_warnings=["ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ã®ãŸã‚å°‚é–€å®¶ã«ç›¸è«‡ã—ã¦ãã ã•ã„"],
                final_answer=f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                recommendation="ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ã®ãŸã‚å°‚é–€å®¶ã«ç›¸è«‡ã—ã¦ãã ã•ã„"
            )

    def _generate_answer(self, candidate_species: List[str], toxicity_info: Dict, 
                        cooking_methods: Dict, user_question: Optional[str]) -> str:
        """æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆ"""
        if not candidate_species or candidate_species[0] == 'ä¸æ˜':
            return "ç”»åƒã‹ã‚‰ã‚­ãƒã‚³ã®ç¨®é¡ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å°‚é–€å®¶ã«ç›¸è«‡ã—ã¦ãã ã•ã„ã€‚"
        
        answer = f"ä¸Šä½å€™è£œç¨®: {', '.join(candidate_species[:3])}\n\n"
        
        # å®‰å…¨æ€§æƒ…å ±
        has_toxicity = any(
            any('toxic' in doc.get('content', '').lower() or 'poisonous' in doc.get('content', '').lower() 
                for doc in docs) 
            for docs in toxicity_info.values()
        )
        
        if has_toxicity:
            answer += "âš ï¸ æ³¨æ„: æ¯’æ€§ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚çµ¶å¯¾ã«æ‘‚å–ã—ãªã„ã§ãã ã•ã„ã€‚\n\n"
        else:
            answer += "â„¹ï¸ æ¯’æ€§æƒ…å ±ã¯ç¢ºèªã§ãã¾ã›ã‚“ã§ã—ãŸãŒã€æœ€çµ‚åˆ¤æ–­ã¯å°‚é–€å®¶ã«å§”ã­ã¦ãã ã•ã„ã€‚\n\n"
        
        # èª¿ç†æƒ…å ±
        if cooking_methods:
            answer += "ğŸ³ èª¿ç†æƒ…å ±:\n"
            for species, docs in cooking_methods.items():
                if docs:
                    answer += f"- {species}: èª¿ç†æ³•ãŒç¢ºèªã§ãã¾ã™\n"
        
        answer += "\nâ€¼ï¸ é‡è¦: ã©ã‚“ãªã‚­ãƒã‚³ã§ã‚‚ã€å°‚é–€å®¶ã®ç¢ºèªãªã—ã§ã®æ‘‚å–ã¯å±é™ºã§ã™ã€‚"
        
        return answer
    
    def _generate_recommendation(self, candidate_species: List[str], safety_warnings: List[str]) -> str:
        """æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        if safety_warnings:
            return "æ¯’æ€§ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€çµ¶å¯¾ã«æ‘‚å–ã›ãšå°‚é–€å®¶ã«ç›¸è«‡ã—ã¦ãã ã•ã„ã€‚"
        elif not candidate_species or candidate_species[0] == 'ä¸æ˜':
            return "è­˜åˆ¥ãŒå›°é›£ãªãŸã‚ã€åˆ¥ã®è§’åº¦ã‹ã‚‰ã®å†™çœŸã‚’æ’®å½±ã—ã€å°‚é–€å®¶ã«ç›¸è«‡ã—ã¦ãã ã•ã„ã€‚"
        else:
            return "æ¯’æ€§æƒ…å ±ã¯ç¢ºèªã§ãã¾ã›ã‚“ã§ã—ãŸãŒã€å®‰å…¨ã®ãŸã‚å¿…ãšå°‚é–€å®¶ã«ç¢ºèªã—ã¦ã‹ã‚‰åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚"


#%%
# ãƒ¢ãƒã‚¤ãƒ«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼åˆæœŸåŒ–
# ä¸€æ™‚çš„ã«SentenceTransformerã‚’åˆæœŸåŒ–
from sentence_transformers import SentenceTransformer
temp_embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

mobile_workflow = MobileMushroomWorkflow(
    model=model,
    tokenizer=tokenizer,
    embedding_model=temp_embedding_model
)

# ç”»åƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–
mobile_workflow.initialize_image_database(dir0, class_names, paths, classes)

#%%
# 3ã‚¹ãƒ†ãƒƒãƒ—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ - ç°¡æ½”ç‰ˆ
print("ğŸ§ª 3ã‚¹ãƒ†ãƒƒãƒ—ã‚­ãƒã‚³è­˜åˆ¥ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ ãƒ†ã‚¹ãƒˆé–‹å§‹")
print("=" * 80)
print("3ï¸âƒ£ æ³¨ï¼šã“ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¯3ã‚¹ãƒ†ãƒƒãƒ—ã§å®Ÿè¡Œã—ã¾ã™")
print("   1.ç”»åƒãƒãƒƒãƒãƒ³ã‚° â†’ 2.Wikipediaæ¤œç´¢ â†’ 3.å›ç­”ç”Ÿæˆ")

# ãƒ†ã‚¹ãƒˆè¨­å®š
test_question = "ã“ã®ã‚­ãƒã‚³ã¯é£Ÿã¹ã‚‰ã‚Œã¾ã™ã‹ï¼Ÿ"
print(f"ğŸ“¸ ãƒ†ã‚¹ãƒˆç”»åƒ: {image_path}")
print(f"â“ ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•: {test_question}")
print("=" * 80)

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
try:
    print("\nğŸš€ 3ã‚¹ãƒ†ãƒƒãƒ—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œé–‹å§‹...")
    # è©³ç´°ãªé€²æ—è¡¨ç¤ºã§ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ
    result = mobile_workflow.process_image(image_path, test_question, verbose=True)
    
    print("\n" + "=" * 80)
    print("ğŸ“Š 3ã‚¹ãƒ†ãƒƒãƒ—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œçµæœ")
    print("=" * 80)
    
    # çµæœè¡¨ç¤º
    print(f"ğŸ”¬ å€™è£œç¨®: {', '.join(result.candidate_species)}")
    print(f"ğŸ“ˆ é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢: {[f'{s:.3f}' for s in result.similarity_scores]}")
    print(f"âš ï¸ å®‰å…¨è­¦å‘Š: {len(result.safety_warnings)}ä»¶")
    for warning in result.safety_warnings[:3]:
        print(f"   - {warning}")
    print(f"ğŸ’¬ æœ€çµ‚å›ç­”:")
    print(f"   {result.final_answer}")
    print(f"ğŸ¯ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
    print(f"   {result.recommendation}")

except Exception as e:
    print(f"\nâŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {str(e)}")
    print("ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
    import traceback
    traceback.print_exc()

print("\nğŸ 3ã‚¹ãƒ†ãƒƒãƒ—ãƒ†ã‚¹ãƒˆå®Œäº†")

#%%
# è¿½åŠ ãƒ†ã‚¹ãƒˆ: 3ã‚¹ãƒ†ãƒƒãƒ—è¤‡æ•°è³ªå•ãƒ‘ã‚¿ãƒ¼ãƒ³
print("\n" + "=" * 80)
print("ğŸ”¬ 3ã‚¹ãƒ†ãƒƒãƒ—è¿½åŠ ãƒ†ã‚¹ãƒˆ: è¤‡æ•°è³ªå•ãƒ‘ã‚¿ãƒ¼ãƒ³")
print("=" * 80)
print("ğŸ“Š å„ãƒ†ã‚¹ãƒˆã§3ã‚¹ãƒ†ãƒƒãƒ—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œã—ã€ã‚·ãƒ³ãƒ—ãƒ«ãªçµæœã‚’å‡ºåŠ›")

test_questions = [
    "ã“ã®ç¨®é¡ã¯ä½•ã§ã™ã‹ï¼Ÿ",
    "æ¯’æ€§ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
    "èª¿ç†æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„",
    None  # è³ªå•ãªã—
]

for i, question in enumerate(test_questions, 1):
    print(f"\n--- 3ã‚¹ãƒ†ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ {i}/4 ---")
    print(f"è³ªå•: {question if question else '(è³ªå•ãªã—)'}")
    
    try:
        # ã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè¡Œï¼ˆverbose=Falseï¼‰
        start_time = time.time()
        result = mobile_workflow.process_image(image_path, question, verbose=False)
        execution_time = time.time() - start_time
        
        print(f"âœ… å®Ÿè¡ŒæˆåŠŸ (å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’)")
        print(f"   ğŸ”¬ å€™è£œç¨®: {result.candidate_species[:2]}")  # ä¸Šä½2ã¤
        print(f"   ğŸ“ˆ é¡ä¼¼åº¦: {[f'{s:.2f}' for s in result.similarity_scores[:2]]}")
        print(f"   âš ï¸ è­¦å‘Š: {len(result.safety_warnings)}ä»¶")
        print(f"   ğŸ¯ æ¨å¥¨: {result.recommendation[:50]}...")
                
    except Exception as e:
        print(f"âŒ å®Ÿè¡Œå¤±æ•—: {str(e)}")

print("\nğŸ¯ å…¨3ã‚¹ãƒ†ãƒƒãƒ—ãƒ†ã‚¹ãƒˆå®Œäº†")
print("ğŸ“Š å„ãƒ†ã‚¹ãƒˆã§ã‚·ãƒ³ãƒ—ãƒ«ãªæ§‹é€ åŒ–çµæœãŒæ­£å¸¸ã«ç”Ÿæˆã•ã‚Œã¾ã—ãŸ")

#%%
# Vision Fine-tuningä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
print("\n" + "=" * 80)
print("ğŸ¯ Vision Fine-tuningä½¿ç”¨ä¾‹")
print("=" * 80)
print("ğŸ“‹ ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã‚­ãƒã‚³ç”»åƒã§ã®Vision Fine-tuningã®ä½¿ç”¨æ–¹æ³•ã‚’ç¤ºã—ã¾ã™")

# Fine-tuningå®Ÿè¡Œä¾‹ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ - å®Ÿéš›ã«å®Ÿè¡Œã™ã‚‹å ´åˆã¯æœ‰åŠ¹åŒ–ï¼‰
"""
# Vision Fine-tuningå®Ÿè¡Œä¾‹:
print("\\nğŸ”§ Vision Fine-tuningå®Ÿè¡Œä¾‹:")
print("# 1. Fine-tuningã‚’å®Ÿè¡Œ")
print("mobile_workflow.run_vision_fine_tuning(")
print("    image_paths=paths,")
print("    labels=classes,") 
print("    class_names=class_names,")
print("    output_dir='./mushroom_vision_finetuned',")
print("    max_samples_per_class=5,  # å„ã‚¯ãƒ©ã‚¹5æšã¾ã§")
print("    num_epochs=2  # çŸ­æ™‚é–“ãƒ†ã‚¹ãƒˆç”¨")
print(")")
print("")
print("# 2. Fine-tunedãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹ç¢ºèª")
print("mobile_workflow.get_model_status()")
print("")
print("# 3. ä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆæ¬¡å›èµ·å‹•æ™‚ï¼‰")
print("mobile_workflow.load_fine_tuned_model('./mushroom_vision_finetuned')")
print("")
print("# 4. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«æˆ»ã™")
print("mobile_workflow.switch_to_base_model()")
"""

# ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ç¢ºèª
print("\nğŸ“Š ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹:")
status = mobile_workflow.get_model_status()

# Fine-tuningæº–å‚™æƒ…å ±
print(f"\nğŸ”§ Fine-tuningæº–å‚™çŠ¶æ³:")
print(f"   åˆ©ç”¨å¯èƒ½ç”»åƒæ•°: {len(paths)}")
print(f"   åˆ©ç”¨å¯èƒ½ã‚¯ãƒ©ã‚¹æ•°: {len(class_names)}")
print(f"   æ¨å¥¨è¨­å®š:")
print(f"     - max_samples_per_class: 5-10 (VRAMåˆ¶ç´„ã®ãŸã‚)")
print(f"     - num_epochs: 2-3 (éå­¦ç¿’é˜²æ­¢)")
print(f"     - batch_size: 1-2 (VRAMåˆ¶ç´„ã®ãŸã‚)")

print(f"\nğŸ’¡ Fine-tuningå®Ÿè¡Œæ‰‹é †:")
print(f"   1. mobile_workflow.run_vision_fine_tuning() ã§Fine-tuningå®Ÿè¡Œ")
print(f"   2. è‡ªå‹•çš„ã«Fine-tunedãƒ¢ãƒ‡ãƒ«ã«åˆ‡ã‚Šæ›¿ã‚ã‚Šã¾ã™")
print(f"   3. process_image()ã§Fine-tunedãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨ã•ã‚Œã¾ã™")
print(f"   4. mobile_workflow.switch_to_base_model() ã§ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«æˆ»ã›ã¾ã™")

print(f"\nâš ï¸ æ³¨æ„äº‹é …:")
print(f"   - Vision Fine-tuningã¯å¤§é‡ã®VRAMï¼ˆ15GBä»¥ä¸Šï¼‰ã‚’æ¶ˆè²»ã—ã¾ã™")
print(f"   - Colabç„¡æ–™ç‰ˆã§ã¯åˆ¶é™ãŒã‚ã‚‹ãŸã‚ã€å°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å®Ÿè¡Œã—ã¦ãã ã•ã„")
print(f"   - Fine-tuningã«ã‚ˆã‚Šã€ã‚­ãƒã‚³ç”»åƒè­˜åˆ¥ã®ç²¾åº¦å‘ä¸ŠãŒæœŸå¾…ã§ãã¾ã™")

print("\nğŸ Vision Fine-tuningä½¿ç”¨ä¾‹å®Œäº†")
print("ğŸ“š è©³ç´°ãªå®Ÿè£…æ–¹æ³•ã¯ä¸Šè¨˜ã®ã‚¯ãƒ©ã‚¹å®šç¾©ã¨ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‚ç…§ã—ã¦ãã ã•ã„")
